\section{General DOI Data Model}
\label{sec:DOIDataModel}
As described in Chapter~\ref{chap:Intro}, peoples' foveas are guided by visual cues in perceived scenes. To study how people parse a scene, especially from a perceptual (bottom-up) perspective, access to the visual attributes of objects in it (e.g., color, movement) is indispensable. However, visual attributes often encode semantic properties of data (e.g., color may encode disease type in a medical visualization). To hypothesize about cognitive and goal-directed processes that drive visual ones, analysts may wish to investigate directly what data people looked at, as opposed to how the data was shown. This aligns with the top-down theory of visual processing, which implies that it is meaning and significance of content, together with representation, that drives visual scanning. The DOI approach is to capture both visual and semantic data from eye-tracking experiments to support various research questions, such as about perception, cognition, or data exploration and search. 

DOIs are defined to overlap significant chunks of a visualization's underlying data (e.g., a protein in a protein-interaction network), and inherit the semantic data attributes and values that define those chunks (e.g., protein name, type, function). DOIs also include visual attributes that describe how that information is shown on the screen (e.g., shape, the color of protein glyph). A DOI instrumentation will capture for each user fixation; the DOIs the user may have intended to view, potentially along with low-level attributes of the respective fixation. Below we describe a formal model that captures this idea, and exemplify it in the context of the three case studies (Table~\ref{tab:DOIDataExamples}).

We approximate a visualization's data using the generic entity-attribute-value (EAV) data model~\cite{deran1991entity}, in which combinations of attribute-value pairs describe entities.  We thus define data as the set $D$, containing $N\!d$ data entities $d$. Each entity is itself defined by multiple pairs of data attributes ($da$) and data attribute values ($dav$):
%%\vspace{-0.5mm}
\begin{equation*}
\begin{split} 
D = \{d_i \,|\, i=1..N\!d\} \\
d_{i} = \{da_{i,k} = dav_{i,k} \,|\, k=1..N\!da_{i}\}
\end{split}
\end{equation*}

The definition above describes static datasets. In real applications, data can change over time as a result of user interaction (e.g., a user changes the speed of a vehicle in a 3D simulation; user annotates or deletes data in a visualization). Again, as a result of factors external to the visualization (e.g., data are streamed from a simulation). We augment the definition to include a temporal domain $T$ (e.g., the time of the eye-tracking experiment):
%%\vspace{-0.5mm}
\begin{equation*}
\begin{split}
D_t = \{d_{i,t} \,|\, i=1..N\!d_t, t \in T\} \\
d_{i,t} = \{da_{i,t,k} = dav_{i,t,k} \,|\, k=1..N\!da_{i,t}\}
\end{split}
\end{equation*}

Visualizations turn data sets into visual models by defining visual elements to represent data elements. While this mapping is often one to one, this is not necessarily true; a single visual representation might capture multiple data elements. As such, we will define a visual model as the set $M$, containing $N\!m$ visual entities $m$. Each visual entity contains a reference to one or more data entities it depicts, and a collection of visual attributes and values that define it (e.g., position, shape, size, color). As before, our definition accounts for possible changes over time, as users may change a visualization through interaction.  

\vspace{-4mm}
\begin{multline*}
\qquad\qquad\quad\; M_t = \{m_{i,t} \,|\, i=1..N\!m_t, t \in T\} \\
m_{i,t} = \{\,\{d_{j,t}\,|\, j=1..N\!md_{i,t}\}, \\ ma_{i,t,k} = mav_{i,t,k} \,|\, k=1..N\!ma_{i,t}\}
\end{multline*}

Finally, models are rendered on the screen via a transformation (e.g., dependent on zooming and panning in 2D visualizations) or projection (e.g., dependent on perspective changes in 3D). The mapping between model and screen entities is generally one to one (i.e., one screen entity for each model entity), but attribute values may differ between model and screen entities even when attribute names are similar. For example, an entity's size in model space is often not the same as in screen space. So, we define our screen visualization as a set of screen entities, same in number as model entities, with one associated model entity, and pairs of screen attributes and values. Screen attributes can include screen-capture cutouts of individual DOIs.

\vspace{-3mm}
\begin{multline*}
\qquad\qquad\; S_t = \{s_{i,t} \,|\, i=1..N\!m_t, t \in T\} \\
s_{i,t} = \{m_{i,t}, \,\, sa_{i,t,k} = sav_{i,t,k} \,|\, k=1..N\!sa_{i,t}\}
\end{multline*}

Eye-trackers report fixations periodically, as time-stamped 2D coordinates with an associated duration. Fixations may be described by additional properties such as dispersion or pupil size.So, we define fixations reported during an experiment as:

\vspace{-5mm}
\begin{multline*}
F_t = \{x, y, duration, fa_{k} = fav_{k} \, | \, k=1..N\!fa_t\}, \, t \in T
\end{multline*}

Both Bernhard et al.~\cite{Bern14} and our method (Chapter~\ref{chap:DOIDataCollection}) compute candidate objects a user is likely to have viewed during a fixation by considering not just the fixation point, but also a small area around it. If multiple DOIs intersect that area, Bernhard et al. report only the object closest to the fixation, while our method report all viewing candidates.  For a more general DOI data model, we will consider the approach we discussed in Chapter~\ref{chap:DOIDataCollection}. Thus, each fixation may be associated with multiple viewed DOIs, and the confidence that a DOI was indeed the locus of a user's attention is proportional to the proximity of the fixation to the DOI (Chapter~\ref{chap:DOIDataCollection}). For maximum flexibility, DOI instrumentations can record the distance of users' fixations to all DOIs, postponing its interpretation (i.e., should an object be considered viewed or not viewed given a specific distance) until the analysis stage: 


$$
d(\,F_t, s_{i,t}\,) = 
\begin{cases}
pixels\,from\,F\,to\,center\,or\,border\,of\,s, & \\ \qquad if\,s\, is \, visible\,on\,screen \\ 
\infty \, , \, otherwise 
\end{cases}
$$

This definition allows us to capture not just DOIs users viewed in an experiment, but also which DOIs were visible and not visible during the experiment and when. This allows analysts to understand not just what viewers chose to view, but also what they chose to ignore. 

Finally, a typical eye-tracking experiment captures DOI sequences for multiple users, as well as data describing those users individually. Since through interaction users can change both the data and how it is displayed, all already introduced definitions should be augmented to reflect that they are user specific. User data ($U$) includes for each user ($u_i$) background and demographic information (e.g., education background, level of expertise, gender), but also user performance data (e.g., answers to questionnaires). Such user attributes could be time dependent (e.g., self-reported fatigue):

\vspace{-5mm}
\begin{equation*}
\begin{split}
U_t = \{u_{i,t} \,|\, i=1..N\!u_t, t \in T\} \\
u_{i,t} = \{ua_{i,t,k} = uav_{i,t,k} \,|\, k=1..N\!ua_{i,t}\}
\end{split}
\end{equation*}

Using the definitions introduced above, DOI data can be formallly described as: 

\vspace{-5mm}
\begin{multline*}
DOI_{t,u} = \{S_{t,u} (+\,linked\, M, D), \\ d(F_{t,u}, S_{t,u}), F_{t,u}, u_{u,t}\} \,|\,t \in T, u = 1..N\!u
\end{multline*}

That is, DOIs are analytic constructs that overlap data elements or subsets and are characterized by four types of attributes: data, visual, user, and perceptual. Examples are shown in Table~\ref{tab:DOIDataExamples}. 
These attributes may be time and user dependent, capturing that in real-life visualizations data and visual encoding change in response to user interactions and external factors.  The model defines data that can be collected in an eye-tracking experiment exhaustively and can thus underlie a broad range of research questions. Visual attributes can reveal the perceptual mechanisms that compel peoples' foveas to fixate on specific visual objects (bottom-up perception). Data attributes may better reveal why users intently choose to look at particular data, and could provide insight into cognitive processes associated with top-down perception. User attributes can tie perceptual and cognitive patterns to user demographics, abilities, and performance. Moreover, the model can be extended as necessary with additional types of attributes, such as modality (e.g., audio) or interaction annotations (e.g., is an object the target of an interaction).

%
\begin{table}[htbp]	
\caption{Example DOIs and attributes collected in each of the three applications described in Chapter~\ref{chap:CaseStudies}.}
    \label{tab:DOIDataExamples}
	\centering
    \begin{tabular}{|c|c|c|}
    \hline
    Visual Analytics & Learning Education & Construction \\\hline
    DOI (at fixation $N$) & DOI (at fixation $N$) &DOI (at fixation $N$)\\
    \begin{tabular}{l}
         \textbf{data attributes}\\
                 type       :  movie\\
            label   :  The Dark Knight\\
            rating: $9.0$\\
     \textbf{visual attributes}\\
                visible : yes\\
                pos      :  $550,300$ (px) \\
                size      :  $200,150$ (px)\\
     \textbf{user attributes}\\
                id                   :  user1  \\
                level              :  graduate\\
                background :  computer \\science\\
                
    \textbf{perceptual attributes}\\
                fix\_pos       : $450,280$ (px)\\
                fix\_spread : $30,25$ (px)\\
                distance     : $20$ (px)\\
                time            : $720,000$ (ms)\\
                duration     : $300$ (ms)
	\end{tabular}
    & 
    \begin{tabular}{l}
         \textbf{data attributes}\\
                type       :  definition\\
                format   :  text\\
                concept :  structure\\
                level       :  intro\\
     \textbf{visual attributes}\\
                visible : yes\\
                pos      :  $120,300$ (px) \\
                size      :  $300,100$ (px)\\
     \textbf{user attributes}\\
                id                   :  user1  \\
                level              :  senior\\
                background :  arts\\
                accuracy       :  85 ($\%$)\\
    \textbf{perceptual attributes}\\
                fix\_pos       : $130,280$ (px)\\
                fix\_spread : $30,25$ (px)\\
                distance     : $20$ (px)\\
                time            : $51,000$ (ms)\\
                duration     : $280$ (ms)
	\end{tabular}
    &
    \begin{tabular}{l}
     \textbf{data attributes}\\
            type       :  worker\\
            helmet   :  yes\\
            size         : $0.7,0.4,1.8$ (m)\\
            moving :  $1.5$ (m/s)\\
            hazard  :  \\caught in between\\
 \textbf{visual attributes}\\
            visible              :  yes\\
            pos                   :  $560,430$ (px) \\
            size                   :  $20,40$ (px)\\
            color                :  $(100,150,150)$\\
            appearance    :  image ref\\
 \textbf{user attributes}\\
            id                   :  user1  \\
            experience   : $5$  (years) \\
            background :  construction\\
            accuracy       :  $7$ \\(hazards spotted)\\
\textbf{perceptual attributes}\\
            fix\_pos       : $130, 280$ (px)\\
            fix\_spread : $30,25$ (px)\\
            distance     : $20$ (px)\\
            time            : $51,000$ (ms)\\
            duration     : $280$ (ms)
\end{tabular}\\
\hline
	\end{tabular}
    \vspace{3mm}
    
	
\end{table}