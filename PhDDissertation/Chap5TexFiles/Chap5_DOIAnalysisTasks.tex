\section{Possible and Probable DOI Tasks}
\label{sec:DOIAnalysisTasks}

\subsection{Related Work}
AOI analysis task categorization is non-existent. In this section, we discuss related works on contributions regarding task taxonomies and frameworks and draw inspiration from the methods involved in their creation. We primarily focus on task taxonomies for spatio-temporal data by Andrienko et al.~\cite{And03}, and cartography and geo-visualization by Roth~\cite{Roth13}. Similar to these works, we aim to generate and categorize DOI tasks by considering data-derived goals (i.e., high-level, domain specific questions that analysts would like to answer), operands (i.e., the specification and answer of a data question), and objectives (i.e., low level analytic question on the data). 

Previous efforts found it useful to define tasks in terms of their operands, data categories that can be used as inputs (i.e., task specification) and outputs (i.e., answer) to a task.  For example, in the context of geographical data, Andrienko et al defined three basic operands: space (where), time (when), and object (what). Using these operands, Andrienko et al.'s defined three basic kinds of possible questions, in terms of inputs and outputs, for spatio-temporal analyses: $when + where \rightarrow what$, $when + what \rightarrow where$, and $where + what \rightarrow when$. We aim to employ a similar analysis in the context of the formal DOI data model. Therefore, an example of a ($when + where \rightarrow what$) question in our particular domain could be ``Which character (What) was viewed in Time T (When) in `Fantine Cluster' (Where) in the Les Miserables Graph (Figure~\ref{fig:Miserables})?''. However, our data model is likely to differ from that of Andrienko (e.g., by including different types of operands), and as such the space of possible tasks will differ as well. 

Additionally, existing work also found it useful to define tasks in terms of their objective primitives, or what type of cognitive task they involve. For example, Roth's five objective primitives include: identify (i.e., find a piece of information given some other piece of information), compare (i.e., compare two pieces of information of the same time), rank (i.e. determine order of information pieces), associate (i.e. capture relationships between different information pieces), and delineate (i.e. group or cluster information pieces)~\cite{Roth13}.  Roth's objective primitives are validated empirically and are thus well suited to categorize both loosely defined and specific objectives. We combined these objectives with our operand models to define the possible range of possible DOI tasks. For example, a specific task involving these operands and this objective would be ``Which character was viewed most (objective: rank) by user A (operand: who)?''. 

\subsection{Objectives and Probable Data Questions}
After reviewing multiple taxonomies and frameworks, we decided to use Roth's five objective primitives --- \textit{identify, compare, rank, associate, delineate} ~\cite{Roth13}  --- for two reasons. First, these objectives were validated empirically and shown to correlate with how real users think about the tasks they are doing. Second, they are a compromise between loosely defined objectives with broad meaning, and very specific objectives. For example, Andrienko et al. define only two cognitive objectives, identify and compare. While these are indeed sufficient to describe Roths primitives (e.g., an association is a comparison of attributes), we think Roth's more specific objectives map to analysts' goals more directly, making it easier to consider possible tasks in practice. Conversely, Amar et al. define very specific objectives which we felt occasionally overlapped and made it difficult to map concrete data questions to single objectives~\cite{Ama05}. 

\vspace{2mm}

\noindent\textbf{Identify} allows an analyst to extract a data characteristic from a given data target. After considering possible tasks and how they support high-level research goals in our three concrete applications, we distilled the probable types of questions listed below. These essentially boil down to identifying what data a group of subjects viewed and how (e.g., time, gaze properties), which subjects viewed certain data at certain time, and what subjects' characteristics are. They also account for the fact that analysts may wish to focus their data questions on specific users or groups of users (e.g., students with an engineering background), an experiment's entire duration or just a temporal subset (e.g., second task, the first minute of each task), and on specific subsets of data (e.g., definitions, fast moving machinery). 

\vspace{2mm}
\hangindent=3mm\textit{I1: During all or part of the experiment, one ore more subjects looked at data or a specific subset of data --- (i) with what data or visual attributes; (ii) and/or when, how long; (iii) and/or how often; (iv) and/or in what way?}

\vspace{2mm}
\hangindent=3mm\textit{I2: During all or part of the experiment, what are the attributes of subjects that viewed a specific subset of data --- (i) in a particular way; (ii) and/or at a particular time; (iii) and/or particularly often?} 

\vspace{2mm}
\hangindent=3mm\textit{I3: During all or part of the experiment, what are the attributes of one or more given subjects?} 


\vspace{2mm}
\noindent\textbf{Compare} captures the objective of determining the differences or similarities between two data targets. It is possible for two compared targets to have the same form but a different level of generality: "Did user A look at different things than everyone else in task one?". We identified the following probable compare objectives: 

\vspace{2mm}
\hangindent=3mm\textit{C1: Compare individual or groups of subjects, based on all or a subset of data they saw or accessed, during all or a part of the experiment, by --- (i) the data or visual attributes of that data; (ii) and/or when, how long, or how often they looked at it or it was visible; (iii) and/or how they looked at it.}

\vspace{2mm}
\hangindent=3mm\textit{C2: Compare time subsets, based  on all or a subset of the data viewed or accessed by one or a group of subjects in those times, by --- (i) the data or visual attributes of that data; (ii) or when, how long, or how often the data was viewed or it was visible; (iii) or how the data was viewed; (iv) or the attributes of the users that viewed or accessed it.}

\vspace{2mm}
\hangindent=3mm\textit{C3: Compare subsets of data, viewed or accessed by one or a group of subjects, during all or part of the experiment, by --- (i) its data or visual attributes; (ii) or when, how long, how often it was viewed; (iii) or how it was viewed; (iv) or the attributes of the users that viewed or accessed it.}
	
\vspace{2mm}
\hangindent=3mm\textit{C4: Compare individual or groups of subjects based on their properties, during all or part of the experiment.}

\vspace{2mm}
\noindent\textbf{Rank} allows analysts to determine the order of multiple objects. The space of probable ranking questions is similar to that of comparison questions, only involving more than two operands. It is important to note that ranking operations, by Roth's definition, will include questions pertaining to the identification of extremums, outliers, and means and centroids. A few examples of concrete ranking tasks are shown in Table~\ref{tab:Tasks}. 


\vspace{2mm}
\noindent\textbf{Associate} allows analysts to capture the relationship between different attributes, and is synonymous with the correlate objective in other taxonomies. To describe associate tasks we need to consider the two characteristics to compare, and the data subset that they are sought in. As Andrienko et al. point out~\cite{And03}, and we observed in practice, it is rare that association task would be performed across different targets. As such, we identified the following probable associate objectives:

\vspace{2mm}
\hangindent=3mm\textit{A1: Are there correlations between attributes of all or a group of subjects, and --- (i) data or visual properties of; (ii) when, how long, or how often; (iii) how --- data or subsets of data those subjects viewed or accessed during all or part of the experiment?} 

\vspace{2mm}
\hangindent=3mm\textit{A2: Are there correlations between when, how long, or how often data or subsets of data that one or more subjects viewed or accessed during all or part of the experiment and --- (i) data or visual properties of that data; (ii) how that data was viewed?}

\vspace{2mm}
\hangindent=3mm\textit{A3: Ar there correlations between attributes of all or a subset of data and how that data was viewed by one or more subjects during all of part of the experiment?}

\vspace{2mm}
\hangindent=3mm\textit{A4: Are there correlations between the attributes of all or a group of subjects, during all or part of the experiment?}

\vspace{2mm}
\hangindent=3mm\textit{A5: Are there correlations between the attributes of data or subsets of data viewed or accessed by one or more subjects during all or part of the experiment?}

\vspace{2mm}
\hangindent=3mm\textit{A6: Are there correlations between when and how long or how often data or subsets of data were viewed or accessed, by one or more subjects, during all or part of the experiment?}

\vspace{2mm}
We note that the phrasing 'are there correlations', which denotes a confirmatory goal, can be changed to 'find correlations', which denotes a more general, exploratory goal.

\vspace{2mm}
\noindent\textbf{Delineate} tasks capture analysts' objective of organizing data in logical structures, such as clusters or groups. Delineate tasks operate on the same operands as compare and rank tasks. 

\begin{sidewaystable}
%\begin{table}[htbp]	
	\centering
    \begin{tabular}{|l|l|}
    \hline
    Task Type & Task Instance \\
    \hline
        I(i)& What was the type distribution of advanced architecture concepts that subjects looked at?\\
    I1(ii) &Cumulatively, how much time did subject X spend looking at moving objects?\\
    I1(iv) &On average, how close were experienced users’ fixations from the center of the closest object?\\
    I2(ii) &Which subject looked at definition X in the first minute of the experiment?\\
    I2(iii) & What is the average experience of subjects who looked at\\& every object associated with `caught in between' hazards at least twice?\\
    I3 & How fatigued did subject X report to be at the end of the study?\\
    \hline
    C1(ii) & Do experienced users view hazard-tagged objects faster once they become visible, than do novices?\\
C1(iii) & Do experienced users fixate closer to objects than do novices?\\
C2 (i) & When do subjects look at genres more, in the beginnings or at the ends of tasks?\\
C2 (iii) & Do subjects fixate closer to objects in the first minute of a task than in the last minute of it?\\
C3(i)& What distinguishes visible data that subjects looked at, from visible data that they ignored?\\
C3(iii)& Are examples being viewed more than definitions by experienced users?\\
C4 & Are our experienced users typically older than our novices?\\
    \hline
R1(i) & Which user tends to look at examples first?\\
R3(ii) & What do users look at most in the first few seconds after spotting a new movie: actors, directors, genres, or ratings?\\
R3 (ii) &What type of learning object do successful learners look at most?\\
R2 (ii)&  During which task did subjects start looking at examples earliest?\\
R3(ii) &Which one object was viewed most by experienced subjects in the third section of the experiment?\\
R4 & Which user was the most successful learner?\\
    \hline
    A1 (i)& Is there are correlation between the background of subjects (e.g., science) \\&and the format of learning content they focus on (e.g. numeric)?\\
A2 (ii)& Do people fixate further away from objects as time progresses in a task?\\
A2 (i) & Is there are correlation between how near objects are to a subject and how much subjects focus on them?\\
A3 &  Do subjects tend to fixate closer to objects that appear smaller on the screen?\\
A4& Is users’ experience correlated with their ability to identify more hazards? \\
A5& Is there a correlation between the genres and ratings of movies that subjects viewed? \\
A6& Do effective learners look at examples more as time progresses?\\
\hline
D1 (i + ii) & Cluster subjects based on the what content they viewed, and when.\\
D2 (i) & Cluster tasks based on how the content viewed in them. \\
D3 (iv) &Cluster the objects tracked in the experiment by \\&the attributes of the users who viewed them (e.g., their experience, their performance).
\\
D4 & Cluster subjects based on their attributes. \\
\hline
%\vspace{0.1mm}
    \end{tabular} 
    \caption{DOI task examples.}
    \label{tab:Tasks}
\end{sidewaystable}
%\end{table}

