\chapter{BACKGROUND}
\label{chap:Foundations}
\section{Origin of Eye-Tracking}
Eye-trackers provide a stream of gaze points based on the subtle positions changes of eye-pupils. However, the pattern of human perception reveals that stream of gaze points are not smooth trajectories. Earlier, in the 1800s, eye movements were interests affiliated to know how people read.  Javal~\cite{javal1878essai}, Lamare~\cite{lamare1893mouvements}, and  Hering~\cite{hering1879raumsinn} first discovered that people make stops while scanning through words while reading. Thus, eye-movements yield two types of gaze points: fixations, and saccades. Fixations are the points where eye stops moving for a while. On the other hand, saccades are intermediate points where eye stops for a small amount of time during switching fixations from one object to another. Edmund Huey was the first to build an eye-tracking device to track eye movement in reading~\cite{huey1908psychology}. He used lenses with small openings attached to a pointer. Later, Judd and Buswell developed an eye movement camera to capture eye motions~\cite{judd1922silent}. 

With the progress of eye-trackers, it opened paths for more research in different disciplines. In 1967, Yarbus correlated eye-movements with user study tasks~\cite{yarbus1967eye}. Presently, Eye-tracking is a popular tool in many research domains such as psychology, neuroscience, Marketing, human-computer interaction, data visualization~\cite{Duch02}. The applications of eye-tracking technology are discussed in Section~\ref{sec:EyeTrackingApplication}.



\section{Applications of Eye-Tracking}
\label{sec:EyeTrackingApplication}
Eye-tracking technology is gradually getting more accurate, faster, and cheaper~\cite{Duch07}. Due to its availability, more research studies are adopting it as a utility. We can divide the applications of eye-tracking technology into two categories: Interactive, and Diagnostic~\cite{Duch02}. This dissertation primarily focuses on eye tracking's diagnostic role. However, we briefly discuss its interactive applications in Section~\ref{sec:EyeTrackingInteractive}. Later, we discuss the use of eye-tracking as a diagnostic application in Section~\ref{sec:EyeTrackingDiagnostic}. 

\subsection{Eye-Tracking as an Interactive Tool}
\label{sec:EyeTrackingInteractive}
Eye movement is significantly faster than hand movements~\cite{sibert2000evaluation}. Thus, many computer-based systems have used eye-tracking as an interactive tool. Examples include using eye-tracking as an alternative to pointing devices (e.g. mouse, touch-interface) in 2D~\cite{Jacob91} and 3D~\cite{Bolt90, Tan00}, and even as a text-input device~\cite{Maj02}. However, eye-tracking is proved as ineffective compared to traditional selective devices (e.g. mouse, touch, and keyboard) due to the difficulty of differentiating between view-gazes and interaction-gazes. Such case is known as the ``Midas Touch Problem''~\cite{Jacob91}. For example, a Graphic User Interface (GUI)-based system uses eye-tracking as a selective system. The system's screen contains two icons: `A' and `B'. If a user wants to select `B' but looks at `A' (view gaze) then looks at `B' (interaction gaze) then the system may find it ambiguous to decide which icon the user wants to select. To overcome this problem, Jacob proposed several solutions such as use blinks or dwell time. However, using such solutions make eye-tracking interaction slower than traditional interaction methods~\cite{Jacob91}. 

Despite being fast, eyes are not effective to control interactions. Moreover, it cannot fully serve the selective system purpose~\cite{Zha99}. However, gaze points can indicate user's intentions and displays can alter in gradual and unobtrusive nature~\cite{Jacob91, Jacob03}. Such interactive displays are labeled as ``Gaze-Contingent Displays'' (GCD)~\cite{Duch07}.  Researchers developed GCDs by changing either screen contents~\cite{Rein03, PN02} or underlying model before rendering~\cite{Dan00, OD01, ODH02, Okoe14}. 

\subsection{Eye-Tracking as a Diagnostic Tool}
\label{sec:EyeTrackingDiagnostic}
Computer vision has a profound impact on artificial intelligence, medical diagnostics, and visual perception~\cite{forsyth2011computer, Huang:400313}. Examples of computer vision applications include facial recognition~\cite{sirovich1987low}, video scene detection~\cite{yeo1995rapid, gao2006pca}, and disease detection such as Alzheimer's disease~\cite{chen2004automated}, glaucoma~\cite{balasubramanian2009framework}, and retinopathy~\cite{cao2011high, cao2008automated}. Eye tracking technology uses similar methods to computer vision for diagnostic purposes. Many research user studies use eye-tracking as a diagnostic tool. The most common form of a diagnostic eye-tracking study is a user solving visual tasks by observing visual stimuli on a computer screen while an eye-tracker records the user's gaze positions. Then, analyzers process the gaze data offline to understand how the user observed the stimuli and solved the tasks~\cite{Duch07}. In this way, researchers used eye-tracking to understand how people recognize faces~\cite{Guo14, Sha14}, how attention changes with emotion~\cite{Ver13}, how diseases may affect perception~\cite{Kim14}, and how students learn from visual contents~\cite{Zaw15, May10, vGo10, Con13}.

The use of eye-tracking in data visualization research has increased with the growing popularity, accuracy, and affordability of the technology. For example, major contributions building on eye-tracking technology include the network readability study by Pohl et al.~\cite{Poh09} and Huang et al.~\cite{Hua08, Hua05}. Moreover, the study on tree drawing perception by Burch et al.~\cite{Bur11, Bur13}. As well as, the study on decision-making visualization by Kim et al.~\cite{Kim12}.

\section{Analysis of Eye-Tracking Data}
Research studies using diagnostic eye-tracking heavily depend on analyzing eye-tracking data. Studying from the literature, we divide the analysis methods for eye-tracking data into two paradigms: point-based methods and area of interests (AOI)-based methods~\cite{Bla14}. Point-Based analysis methods treat each gaze sample as a discrete point. As such, point-based analyses usually report overall gaze patterns as spatial or temporal distributions of 2D gaze coordinates over visual stimuli. The major weaknesses of this approach include the requirement to show the same set of stimuli to the human subjects (i.e. users) to be comparable, and the obligation to always analyze the collected gaze data in conjunction with the 2D screen capture. Analyzing each stimulus will result in longer analysis time for larger numbers of stimuli. Moreover, analyzers have to relate gazes with the semantic contents of stimuli manually. This approach is highly ineffective in the case of interactive and dynamic stimuli.

Alternatively, analyzers often define AOIs that are relevant to their hypotheses onto a stimulus~\cite{Bla14}. The number of gaze points landing into an AOI can then be computed automatically as a proxy for users' interest in that AOI. Higher level analyses are thus possible. Examples include but are not limited to investigating reading patterns where each word is an AOI~\cite{Bey05, San04}, to observe where and how long users look at visual regions~\cite{Coco09, Kim12}, and to compare interfaces utilizing AOI fixation counts and frequencies~\cite{Coletkin09}. Usually, analyzers define AOIs over stimuli manually, and the process is significantly time-consuming. Thus, the analysis process takes prolonged time with the increasing count of stimuli and visual contents within those. Moreover, the process becomes prohibitively inefficient for interactive and dynamic stimuli (e.g. video) since analyzers have to define AOIs for each frame of a video.    
  
Several solutions are proposed to overcome this weakness. One example is the automatic AOI annotations using gaze clustering algorithms~\cite{Pri00, San04, Dru14}. However, an increase of complexity of visual contents in stimuli may increase the difficulty of the AOI annotation process. Stellmach et al. proposed the object of interests (OOI) concept for 3D stimuli where eye-trackers collect gaze points on the surface of 3D objects available in a scene~\cite{Ste10}. Additionally, Steichen et al.~\cite{Ste13} and Kurzhal et al.~\cite{Kur14} suggested the possibilities of dynamic AOI annotations in the case of computer generated visual contents. However, this concept is still unexplored. This dissertation leverages this concept of dynamic AOI and OOI into developing DOIs. We discuss related works about DOIs in Section~\ref{sec:RelatedWorks}.

\section{Related Work}
\label{sec:RelatedWorks}
Our contributions in this dissertation explore analyses of eye-tracking data in two aspects: collection and interpretation. We discuss related work about them in the following sections.

\subsection{Eye-Tracking Data Collection} 
In Section~\ref{sec:ProblemContribution}, we have mentioned that the Data of Interests (DOI) is an improved solution for eye-tracking data analysis. The idea of DOI originates from the objective of automatically detecting which data objects a user of a visualization views. As such, DOI is the mapping of gaze samples to data objects rather than pixel positions. Recently, Sundstedt et al.~\cite{Sun13} and Bernhard et al.~\cite{Bern14} introduced a process called gaze to object mapping (GTOM) for identifying objects which are targets of users' attentions in 3D virtual environments. This dissertation contributes a similar approach albeit in the context of relating gaze points with semantic contents of network diagram~\cite{Okoe14}. However, relating gaze points with semantic contents of any visualization is non-existent. Salvucci et al. presented a probabilistic approach to predict viewed objects on a computer screen using eye-tracking~\cite{Sal00}. Moreover, Salvucci et al. alongside with Okoe et al.~\cite{Okoe14} indicated that leveraging semantics of visual contents can significantly improve viewed-object predictions. However, both Salvucci et al.'s and Okoe et al.'s contributions were limited to simple visualizations. Salvucci et al. tested their methods over a simple gaze-added WIMP (i.e. Window, Icon, Menu, and Pointer) interface. On the other hand, Okoe et al. explored only network visualization~\cite{Okoe14}. The idea of OOI and GTOM significantly influenced our first contribution. Moreover, One of our methods innovates on existing techniques for mapping gazes to objects by adopting the probabilistic method by leveraging semantic contents of visualizations.  


\subsection{Eye-Tracking Data Interpretation}
Many methods exist to interpret eye-tracking data visually. Blascheck et al. categorized several existing visualization techniques for point-based and AOI-based visualizations~\cite{Bla14}. More complex visual analytics software systems and solutions include those by Andrienko et al.~\cite{And12}, Weibel et al.~\cite{Wei12}, Kurzhal et al.~\cite{Kur14}, and Blascheck et al.~\cite{Bla16}.

However, DOI data can be significantly more granular and larger than AOI data. Moreover, we can associate DOIs with a wealth of directly-derived data attributes from the tracked data. Hence, we have hypothesized that DOI data can answer questions that AOI cannot. Moreover, data interpretations using traditional AOI analysis methods are ineffective for DOI data. This shortcoming motivated our second contribution, with its two sub-goals: determining what questions DOI data can answer that AOI cannot, and creating support for theses questions. 
 
To accomplish the former, we formalized the DOI specific analytical tasks. Any specific categorizations of analysis tasks for eye-tracking data are currently non-existent. However, task categorizations, task taxonomies, and task frameworks do exist for other types of data and analyses. For example, Wehrend and Lewis~\cite{Weh90}, Shneiderman~\cite{Shne96} discussed general features of task taxonomies in the context of data visualization. Recently, Brehmer et al.~\cite{Bre13}, Schulz et al.~\cite{Sch13}, and Rind et al.~\cite{Rind15} proposed a multilevel typology that can be applicable for creating complete task descriptions regardless of domain specifications. Amar et al. provided a comprehensive categorization of low-level tasks~\cite{Ama05}. Moreover, task taxonomies exist for several specific types of data visualizations such as for graph visualizations~\cite{Lee06}, group level graphs~\cite{Sak14}, multidimensional data visualizations~\cite{Ward02}, and geo-temporal data~\cite{And03, Roth13}. Hence, as a part of our second contribution, we draw inspiration from these studies for an attempt to categorize DOI analysis tasks. 

To achieve the interpretation part of our second contribution~\ref{sec:Contribution-2}, we explored the designs of visualizations to perform analysis tasks for DOI. Moreover, we employed existing visual techniques available for AOI analyses. We also adopted interaction techniques from Yi et al.'s taxonomy for information visualization~\cite{Yi07} to support DOI analysis tasks.  
