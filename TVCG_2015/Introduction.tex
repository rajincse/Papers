\section{Introduction}

\IEEEPARstart{E}{ye}-tracking allows us to locate where users are looking on a computer screen~\cite{ware1987evaluation,jacob1991use}, and is used widely in psychology and cognitive science to help researchers understand thought and affect mechanisms~\cite{rayner1995eye}, and in data visualization and human computer interaction (HCI) to explain how people use visual interfaces~\cite{duchowski2007eye}.  To date, eye-tracking data is collected and interpreted as gaze-coordinates in the space of rendered visual stimuli that gazes were recorded for. Relating this data to the semantic content of the stimuli is generally done offline by analysts who inspect gaze heatmaps visually, or define area of interest (AOIs) manually. This process is inefficient for studies involving many subjects, long sessions, and interactive content.

This paper explores a different approach: for visual content that a computer generates (e.g., data visualizations), the layout of the content is known at rendering time. Thus, gaze-coordinates provided by an eye-tracker can be related to rendered visual objects in real-time, yielding an account of which objects a user views at any given time. Specifically, our approach relies on a visualization to communicate to an object-detection module what content it draws on the screen (e.g., shapes and positions of glyphs), and for the module to match incoming gazes to that content. For interactive visualizations, this communication needs to persist throughout their run-time, as any visual changes (e.g., moving a glyph) need to be reported to the object detection module. Programmatically, we implement this communication by inserting calls to the object detection-module directly in the visualization's source code. For example, a rendering instruction used to add or reposition an object is mirrored by a call that informs the object-detection module of this change. 

The approach is limited to visualizations for which the source code is accessible, and involves the overhead of instrumenting visualizations at a low level. These limitations are balanced by several advantages.  First, once instrumented, a visualization can be reused to collect eye-tracking data from different  datasets and multiple usage scenarios, unlike traditional analyses, which would require new AOIs to be defined for new data. Second, data collection is automatic and independent of a user's interactions, thus enabling experimenters to collect data from interactive visualizations in long studies.  Third, the collected data is highly granular (e.g., a small glyph or label, rather than an entire screen region), and is derived directly from the visualization's underlying data (e.g., protein GRB2 in a protein interaction network), thus having semantic meaning without the need for additional coding. We posit that such data can be particularly well suited to explain how people forage for, analyze, and integrate information in complex visual analytics systems and workflows, as opposed to the traditional role of eye-tracking in studying visual perception. 


Our paper contributes to establishing this method of instrumentation in data visualization. First, we introduce and evaluate an algorithm for viewed-object detection that is tailored to visualization content and is more accurate than just binning gazes into AOIs. We also describe a recipe and guidelines for instrumenting visualizations in practice. We build on results by Salvucci, who showed that viewed object detection is more accurate if gazes are ``interpreted intelligently'', by leveraging that users don't view content randomly, but in patterns influenced by their tasks ~\cite{salvucci1999inferring, salvucci2000intelligent}. For example, as shown in Section~\ref{sec:Evaluation}, we found that users look predominantly at highlighted items that are connected to what they already viewed previously. 

Second, we show that our instrumentation approach is feasible, can be used to collect data over long sessions involving open-ended tasks and interactive content, and produces meaningful results. We instrumented an interactive visualization of IMDB data and collected viewed-object data from nine subjects. We show that viewed objects we detected automatically were tightly correlated with the tasks we asked users to do. We also show that differences between manual annotations of five coders who were asked to analyze the raw gaze data, and our automatically collected data, are not greater than differences between the coders' annotations. Moreover, we show that our novel algorithm for detecting viewed objects outperforms two na\"{\i}ve implementations. 

Third, we demonstrate novel analyses that this instrumentation facilitates. Specifically, we show how the collected data allows us to demonstrate the existence of viewing patterns in visualization, and we discuss the range of applications that the method enables.
