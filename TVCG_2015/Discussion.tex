\section{Discussion}
\label{sec:Discussion}
\subsection{Benefits and Applications}
\label{sec:Benefits}

It took each of our coders about six hours to code eighteen minutes of user data ($6$ subjects $\times$ $3$ minutes). Our instrumentation can reduce this overhead and makes it feasible to analyze eye-tracking data from many subjects, using highly interactive content, in long analysis sessions. Since manual coding is not required, data can be analyzed immediately after or even as it is collected.  Moreover, this data has semantic meaning tied to the underlying data of the visualization and can support broader analyses than traditional eye-tracking data. Such analyses, which we exemplified in Sections~\ref{sec:EvalDataCollected} and~\ref{sec:EvalAssumptionAboutViewingTransition}, focus on data and concepts, and differ from current eye-tracking analyses, which generally focus on understanding low-level visual perception in static visualizations.

As such, our approach can be useful in understanding how users forage for, integrate, and hypothesize about data using interactive visualizations. Visual analytics applications could be instrumented to facilitate the exploration of domain expert workflows, of how expertise influences data search and analysis patterns, and of visual strategies associated with successful hypothesis testing. Similarly, instrumenting visual learning environments could lead to insights into how students learn and what makes some learners more effective than others. 

Data produced by our instrumentation can help explain how visualizations support analysis, discovery, and learning, and how they may be changed to be more efficient. Given a particular domain, it would allow us to quantify which data best answers which questions, what types of data are often used together, and how visual widgets are viewed in an analysis process. Section~\ref{sec:Evaluation} exemplifies these types of quantitative and qualitative analyses. 

Viewing data collected automatically during a user's session could be transformed into summaries that capture the user's activity during a day, week, or month.  Such summaries may help refresh the user's memory at a later time, communicate progress to peers or supervisors, and provide useful hints to other users or analysts exploring similar questions in similar data-sets.

Finally, real-time viewed-object detection opens up two distinct opportunities. First, eye-tracking could be used in teaching. Using instrumented learning environments, instructors could track students' progress in lab assignments in real-time to detect students that are not tending to elements crucial for solving assigned problems, and to provide proactive help. Second, it would allow us to create a new generation of gaze-contingent visualizations that could detect in real-time data that is of interest to a user and make recommendations of unexplored data with similar attributes. The decreasing cost of eye-trackers makes it conceivable that they could be included in regular work stations and make such applications real.

\subsection{Limitations}
\label{sec:Limitations}
Our approach is restricted to visualizations with open source code and cannot be used to automate the full range of eye-tracking studies (e.g., analysis of real imagery or of commercial systems).   This problem is inherent to any type of instrumentation. Capturing an application's interaction data, a website's activity, or a network's throughput requires privileged access to those systems. Our approach is intended primarily for creators or owners of data visualizations who wish to understand how their visualizations are used and how to make them more efficient. 

Instrumenting a visualization by altering its source code and defining viewing-transition probabilities involves an overhead. This is also a general instrumentation problem that requires a case by case decision, by considering the tradeoff between the overhead of instrumenting a specific system and the benefits of collecting data from it. We showed in section~\ref{sec:Receipe} that bundling the predictive algorithm into a reusable instrumentation library reduces the cost of instrumentation.    

Finally, as discussed in section~\ref{sec:Receipe}, picking the right parameters to our predictive algorithm may currently be difficult and require pilot studies.  However, we believe visualizations are rarely used in a random fashion and that specific tasks and visual outputs elicit certain gaze patterns. First, most visualizations have a mechanism for highlighting specific elements through interactions or queries, and we showed that such highlighting impacts how people view objects. Second, while we studied connected elements in the context of graphs, establishing visual connections between elements is also employed in brushing and linking or when using leader lines. Third, data in most real-life visualizations can be divided into semantic groups (e.g., actors, movies, directors; protein kinases, protein receptors; conference papers, journal papers) and we hypothesize that viewing transitions between and within such groups are also not random. Fourth, we showed that users identify data connected to their task and then shift their attention repeatedly and almost exclusively within that data group. We hypothesize further research can lead to a more general understanding of such transition probabilities and provide prescriptive guidelines for choosing the parameters that our predictive algorithm relies on.  Moreover, our conceptual framework facilitates exactly this type of unexplored questions in ways previously not possible. 

\subsection{Performance gains by using a predictive approach}
While in our evaluation the gain from using the predictive approach was relatively small ($5\%$), we think that benefits are highly dependent on the type of visualizations that are instrumented, and that some visualizations will benefit more from the predictive approach. This is suggested by results shown in Table~\ref{tab:TransitionFromMovie}, which reveal very strong biases in how people use visualizations (e.g., up to $11$ times more likely to view highlighted objects connected to previously viewed object, than to view random objects). 

The degree to which such viewing patterns can and need to be leveraged predictively depends on the visualization. For example, in our particular case study, the different data categories (i.e., movies, directors, actors, genres) were spatially separated in different panels. As such, if a gaze landed between multiple data objects, these were generally of the same type and our algorithm could not use object category as a discriminator. Instead, in a traditional node link diagram, multiple types of nodes share the same space, and are distinguishable by specific visual attributes or semantic meaning (e.g., proteins in a protein interaction network can be kinases, receptors, etc.). In such a case, an algorithm could use knowledge that a user is currently scanning for a particular type of node, to distinguish between the viewing of nodes that are co-located but from different groups.
 
Generally, a visualization will benefit from our predictive approach if cluttered heterogeneous content shares the same space, and the visualization provides visual and semantic cues that allow users to select subsets of  data that are relevant to particular tasks. Such visualizations are fairly common in real-life analytic applications. If the visual content is sparse and well separated, then computing gaze scores alone is likely sufficient.

\subsection{Evaluating viewed object detection} 
\label{sec:DiscussionEvaluatingViewedObjectDetection}

The dependence on visualization type makes it hard to assess the impact of the predictive method. Moreover, comparing the output of the predictive algorithm to human annotations is questionable since, if coders look at momentary gaze positions rather than try to understand what users aim to do more broadly, their annotation may be closer to our our simpler, probabilistic detection.  This raises an issue about whether human coders can provide a robust ground truth for evaluating techniques such as ours, and whether such ground truths could be improved if eye-tracking data was collected in conjunction with a think-aloud protocol.

First, we note that the quantitative evaluation described in Section~\ref{sec:Evaluation} is one against the state-of-the-art rather than against a ground truth. In other words, we don't claim that our method produces results that are accurate with respect to what people actually looked at, but claim that our method allows us to analyze the data in the same way a human could, only much faster. 

Second, we believe that striving towards a reliable ground truth is slightly misguided in the context of evaluating eye-tracking instrumentation.  People often view elements without consciously realizing it, since vision is by-and-large a subconscious process~\cite{duchowski2007eye}. People are also able to register multiple objects in a fixated region, while not fixating any one object specifically. For example, while reading people often skip short words or syllables, while still registering that they are there. Moreover, for specific tasks, people may think about multiple objects as single data units of analysis. For example, a user of a graph visualization might think in terms of nodes for some tasks (e.g., are two nodes connected?) but may reason in terms of clusters of nodes or cliques for other tasks (e.g., what is the largest clique in the graph?).  As such, we believe that we can generally capture only what subjects see, and that obtaining a ground truth of what subjects actually look at is either unattainable or, if obtained through think allowed protocols or constrained tasks, would not be ecologically valid. As shown by Ogolla, concurrent think-aloud protocols change visual patterns, especially for exploratory tasks~\cite{ogolla2011usability}.

\subsection{Future Work}
\label{sec:FutureWork}
Our work demonstrates and quantifies the existence of gaze transition biases in visualization, but exploring their prevalence remains an open question that is beyond the scope of this paper. Section~\ref{sec:Limitations} lists multiple potential gaze transition patterns that future research can explore, and our framework provides the necessary tools for such work. This research would both deepen our understanding of how visualizations are used and provide guidelines for choosing appropriate inputs to our algorithms. 

More work is also needed to understand the impact of different parameters involved in viewed object detection. For example, how far away from an item can a user fixate and still be considered to be viewing the item? The parameter that captures this in our algorithm is $R$, and, while we use a constant $R$ for all items, this is unlikely the best approach. Based on qualitative observations in the data we collected,  and knowledge of the interplay between peripheral vision and the fovea~\cite{balas2009summary}, we believe users fixate close to items if they are surrounded by clutter,  but exhibit significantly more variability if items are isolated. Thus, we hypothesize that $R$ should adjust itself dynamically based on the clutter of the region that a user is fixating.  

Additional work is also needed to understand how to detect visual objects other than nodes or labels, such as for instance polylines in a parallel coordinate plot, contours in a group or set visualization, or cells in a heatmap. Since distances from gaze samples to objects can be computed even for objects with irregular shapes, Formula 1 could be used to compute gaze scores ($gs$) in such cases as well. However, it is unclear whether this is the best approach, since it is not known exactly how people parse large or complex objects. Do they fixate uniformly across the shape's entire area, and if so, how many fixations are enough to deem an object viewed? Are more fixations needed for irregular shapes as compared to common shapes like ovals or rectangles? For example, we showed previously that we could detect the viewing of edges in a network visualization, but that this required a more complex way of computing $gs$ scores, which relied on an understanding of how users parse lines visually~\cite{okoe2014gaze}.
