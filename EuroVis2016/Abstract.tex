\begin{abstract}
Generally, visual analysis of eye-tracking data is performed by superimposing heatmaps or scanpaths or by defining area-of-interests (AOI) manually onto visual stimuli, which were used for data collection. We recently proposed an alternative approach which involves shifting the focus from where users look to what users look at. By instrumenting the code that transforms a data model into visual content, gaze coordinates reported by an eye-tracker can be mapped directly to granular data shown on the screen. We called this data-of-interest (DOI)-based analysis and showed that it can be done reliably, with limited overhead. This paper shows that while analogue to AOI data, DOI data is significantly different in structure and scale, and that novel analytic tools are required to handle its properties and unlock the analytic opportunities it facilitates. We lay the foundation for such work by formalizing the DOI data model and an associated task taxonomy, and by summarizing visual encodings and interactions principles that can support this taxonomy.
\end{abstract}
