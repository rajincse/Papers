\section{Related Work}
Eye tracking is an effective mechanism to inspect people's visual patterns and attention processes~\cite{jacob1991use}, and is widely used is fields such as psychology, neuroscience, human-computer interaction, and data visualization to investigate how people perceive, react to, and think with visual stimuli~\cite{duchowski2002breadth}. In a traditional eye-tracking user study, a human subject performs visual tasks while their eyes are monitored by an eye-tracking device. For example, experiments were conducted to study how people recognize faces~\cite{guo2014perceiving},  to uncover attention patterns linked to physical discomforts~\cite{vervoort2013attentional}, and how students learn from graphical content~\cite{mayer2010unique}. Within the area of data visualization was used among other, to study how people interpret graph visualizations~\cite{pohl2009comparing, huang2008beyond, huang2005people}, tree-drawing~\cite{burch2011evaluation, burch2013visual}, and for visualization evaluation~\cite{kim2012does}. The breadth and scope of eye tracking research is growing rapidly as eye-trackers become increasingly accurate, fast, and affordable.  

Eye-tracking data is generally analyzed using one of two approaches: point-based and area-of-interest (AOI)-based~\cite{blascheck2014state}. The first revolves around analyzing users' gazes as 2D coordinates in the space of visual stimuli. The disadvantages of this approach are that users need to view the same stimuli in order for their data to be comparable, and that gazes need to be manually related to the semantic content of the stimuli (i.e., matches gazes to certain objects in the image) by viewing gaze heatmaps for instance.  

Alternatively, AOI methods bin individual gazes into AOIs that analysts define manually and which have semantic meaning, and enable higher-level analyses that target the content of visual stimuli. However, the need for defining and reshaping AOIs for each frame viewed during an experiment makes this approach prohibitively time-consuming for experiments with many visual stimuli, and dynamic or interactive stimuli (e.g., videos, real visualization systems).  Several solutions have been proposed to address this limitation. If we assume that AOIs correspond to hotspots of viewing activity, then they could be defined automatically using gaze clustering algorithms~\cite{privitera2000algorithms, santella2004robust, drusch2014analysing}.  However, AOIs defined in this way are not guaranteed to be closely related to the semantic content of viewed stimuli.  For 3D stimuli, Stellmach et al. introduced the concept of object of interests (OOI) where gazes accumulated on surfaces of 3D objects in the scene~\cite{stellmach20103d}. Steichen et al.~\cite{steichen2013user}, and Kurzhals et al.~\cite{kurzhals2014iseecube} imply that AOIs could be dynamically defined when the visualization content is computer generated, as what is shown on the screen is known during rendering, but do not explicitly pursue this approach. 

Our research builds on recent results that advocate for using eye-tracking to detect what users are viewing rather than where they are looking. Sundstedt et al.~\cite{sundstedt2013visual} and Bernhard et al.~\cite{bernhard2014gaze} proposed gaze-to-object mappings (GTOM) in the context of studying human perception in computer generated 3D environments. Their approach relies on mapping gaze-coordinates received from an eye-tracker directly to objects being rendered as part of 3D scenes. More recently, Alam et al.~\cite{alam15analyzing} formalized this approach for general visualization content as data of interest analyses (DOI) in which the rendering code of visual systems is instrumented so as to relate user's gazes directly to the data underlying a visualization. The authors introduced an algorithm for accurate object detection, showed that mapping gazes to DOIs can be done accurately even for granular DOIs (e.g., individual data-objects such as nodes in a network), and described how interactive visualizations can be instrumented with relatively low overhead.

Our first contribution aligns with efforts to categorize general visualization tasks, and to formalize tasks that underlie the analysis of specific types of data. Shneiderman initially described a high level taxonomy of typical user tasks and interactions in data visualizations~\cite{shneiderman1996eyes}. Amar et al. proposed a more detailed classification of visualization task types while exploring workflows typical of multidimensional data analysis~\cite{amar2005low}.  More recently, Brehmer et al. proposed a multi-level typology that can be aid in creating a complete task description~\cite{brehmer2013multi}. At the same time, specific task taxonomies were defined for visualizations of graph-data~\cite{lee2006task}, group-level graphs~\cite{saket2014group}, multidimensional data  visualization~\cite{ward2002taxonomy}, and 3D volume exploration~\cite{lahaclassification}. Task taxonomies help define visualization requirements that drive visualization research~\cite{amar2005low},  and support evaluation by supplying benchmark tasks~\cite{okoe2015graphunit,jianu2014display,saket2014group,ghoniem2004comparison}. However, no task-taxonomies exist for either AOI or DOI analyses, a shortcoming this paper alleviates.   
 
Finally, our work addresses a need for novel visual encodings and analytics techniques for DOI analysis. Many visualizations of AOI data have been proposed and are summarized exhaustively by Blascheck et al.~\cite{blascheck2014state}.  Examples include scan paths, scarf plots, AOI rivers, and AOI transitions matrices. Research also exists on visual analytics principles and systems targeting AOI data, such as those by Andrienko et al.~\cite{andrienko2012visual}, Weibel et al.~\cite{weibel2012let}, Kurzhals et al.~\cite{kurzhals2014iseecube}, and Blascheck et al.~\cite{blascheck2014state}. However, we will show that such methods do not scale to DOI data and are not suited at answering the types of questions that DOI data can answer. Moreover, our paper describes a design space of visual encodings and interaction methods suitable for DOI analyses, structured around the interaction taxonomy of Yi et al.~\cite{yi2007toward}.


