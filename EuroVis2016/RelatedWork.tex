\section{Related Work}
Eye tracking is widely used is fields such as psychology, neuroscience, human-computer interaction, and data visualization to investigate how people perceive, react to, and think with visual stimuli~\cite{duchowski2002breadth}. In a traditional eye-tracking study, subjects performs visual tasks while their eyes are monitored by an eye-tracker. For example, researchers studied how people recognize faces~\cite{guo2014perceiving}, how students learn from graphical content~\cite{mayer2010unique}, or how people interpret graph visualizations~\cite{pohl2009comparing, huang2008beyond} and tree-drawings~\cite{burch2013visual}. The breadth and scope of eye tracking research is growing rapidly as eye-trackers become increasingly accurate, fast, and affordable~\cite{blascheck2014state}.  

Eye-tracking data is generally analyzed using one of two approaches: point-based and area-of-interest (AOI)-based~\cite{blascheck2014state}. The first revolves around analyzing users' gazes as 2D coordinates in the space of visual stimuli. The disadvantages of this approach are that users need to view the same stimuli in order for their data to be comparable, and that gazes need to be manually related to the semantic content of the stimuli (i.e., match gazes to objects in an image) by viewing gaze heatmaps for instance.  

The second bins individual gazes into AOIs that analysts define manually and which have semantic meaning. Using AOIs, higher-level analyses that target the content of visual stimuli are possible. However, the need for defining and reshaping AOIs for each frame viewed during an experiment makes this approach prohibitively time-consuming for experiments with many visual stimuli, and dynamic or interactive stimuli (e.g., videos, real visualization systems).  Several solutions have been proposed to address this limitation. If we assume that AOIs correspond to hotspots of viewing activity, then they could be defined automatically using gaze clustering algorithms~\cite{privitera2000algorithms, santella2004robust, drusch2014analysing}.  However, AOIs defined in this way are not guaranteed to be closely related to the semantic content of viewed stimuli.  For 3D stimuli, Stellmach et al. introduced the concept of object of interests (OOI) where gazes accumulated on surfaces of 3D objects in the scene~\cite{stellmach20103d}. Steichen et al.~\cite{steichen2013user}, and Kurzhals et al.~\cite{kurzhals2014iseecube} imply that AOIs could be dynamically defined when the visualization content is computer generated, but do not explicitly pursue this approach. 

Our research builds on recent results that advocate for using eye-tracking to detect what users are viewing rather than where they are looking. Sundstedt et al.~\cite{sundstedt2013visual} and Bernhard et al.~\cite{bernhard2014gaze} proposed gaze-to-object mappings (GTOM) in the context of studying human perception in computer generated 3D environments. Their approach relies on mapping gaze-coordinates received from an eye-tracker directly to objects rendered as part of 3D scenes. More recently, Alam et al.~\cite{alam15analyzing} formalized this approach for general visualization content as data of interest analyses (DOI), in which the rendering code of visual systems is instrumented so as to relate users' gazes directly to the data underlying a visualization. The authors introduced an algorithm for accurate object detection, showed that mapping gazes to DOIs can be done accurately even for granular DOIs (e.g., individual data-objects such as nodes in a network), and described how interactive visualizations can be instrumented with relatively low overhead.

Our first contribution aligns with efforts to categorize visualization tasks and organize them into task-taxonomies. Shneiderman initially described a high level taxonomy of typical user tasks in data visualizations~\cite{shneiderman1996eyes}. Amar et al. proposed a more detailed classification of visualization task types while exploring multidimensional data analyses~\cite{amar2005low}.  More recently, Brehmer et al. proposed a multi-level typology that can aid in creating a complete task description~\cite{brehmer2013multi}. At the same time, specific task taxonomies were defined for visualizations of graph-data~\cite{lee2006task}, group-level graphs~\cite{saket2014group}, and multidimensional data  visualization~\cite{ward2002taxonomy}. Task taxonomies help define visualization requirements that drive visualization research~\cite{amar2005low},  and support evaluation by supplying benchmark tasks~\cite{okoe2015graphunit,jianu2014display,ghoniem2004comparison}. However, no task-taxonomies exist for either AOI or DOI analyses, a shortcoming this paper alleviates.   
 
Finally, we address a need for DOI visual encodings and analytics techniques. Many visualizations of AOI data have been proposed and are summarized exhaustively by Blascheck et al.~\cite{blascheck2014state}.  Examples include scan paths, scarf plots, AOI rivers, and AOI transitions matrices. Research also exists on visual analytics principles and systems targeting AOI data, such as those by Andrienko et al.~\cite{andrienko2012visual}, Weibel et al.~\cite{weibel2012let}, Kurzhals et al.~\cite{kurzhals2014iseecube}, and Blascheck et al.~\cite{blascheck2016va}. We will show that such methods do not scale to DOI data and are not suited for answering the types of questions that DOI data can answer. Our paper describes a design space of visual encodings and interactions suitable for DOI analyses, structured around the interaction taxonomy of Yi et al.~\cite{yi2007toward}.


