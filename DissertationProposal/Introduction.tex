\chapter{Introduction}
\label{chap:Intro}
Eye-tracking is a method of reporting eye activity using special eye-tracking hardware. Typically, eye-tracking devices can pin-point where a person is looking on a display device (e.g. computer screen, projection screen, handheld, and wearable displays).While eye-tracking can be used both as a diagnostic tool (i.e., describe a user's visual behavior) and an interactive input device (i.e., change an interface based on a user's visual attention), this dissertation will primarily focus on the former. In its diagnostic capacity, eye-tracking technology generally serves the purpose of quantitatively measuring people's attentional process as they solve visual tasks. It plays a major role in research fields such as human-computer interaction, cognitive sciences, and information visualization.  In a typical diagnostic eye-tracking experiment, a human subject is placed in front of a computer screen which shows a visual stimulus. Meanwhile, an eye-tracking device (i.e. eye-tracker) reports and records the subject's gaze positions on the screen. Experimenters then test their hypotheses by analyzing the collected data using visual and statistical analytics techniques.  

Presently, eye-tracking data, collected as a stream of 2D gaze-samples, is analyzed by one of two approaches: point-based and area of interests (AOI)-based analysis. In point-based methods, gaze samples are treated as individual points that are related to the 2D stimulus shown on the screen when the samples were collected. In AOI-based methods, experimenters first define certain regions or areas within the analyzed 2D stimuli. Gaze samples are then binned into those regions or AOIs, which then serve as a higher-level unit of analysis.  

A major limitation of these approaches is that both of them involve a significant overhead: gaze samples are collected as pixel coordinates and experimenters relate them to the visual stimulus by either overlaying gaze-clouds on top of the stimulus image (e.g. heatmap), or by having to manually define AOIs. However, if the stimulus is dynamic or interactive then these analysis actions have to be repeated for each frame of stimulus. This makes such approaches infeasible for dynamic or interactive stimuli. 

A solution presents itself with the realization that in data visualizations the arrangement and layout of visual contents are known at rendering time. Hence, for visualizations with accessible source-code, this can be instrumented so that gaze samples are related to visual contents automatically and in real-time. In other words, we can track what data objects users are viewing at each consecutive moment in time. For example, a network visualization may contain visual representations of nodes and edges. Since we know where these data objects are drawn on the screen, we can map gaze-samples provided by an eye-tracker to them. To exploit the analogy with the established AOI nomenclature, we call such eye-tracked data objects \textit{Data of Interest (DOI)}, and the entire detection and analysis process \textit{DOI eye-tracking analysis}.

The particularity of DOI analysis is that it can be performed in data space rather than image space. In other words, DOIs are coupled with visualization data. Thus, DOIs are intrinsically annotated with data attributes. As a result, DOIs can be analyzed based on its data-derived properties, independently from visual stimuli. Hence, it will eliminate manually relating gaze samples to visual stimuli process which is regularly performed in traditional analysis methods (i.e. point-based and AOI-based). Moreover, DOI analysis will support experiments of significantly longer sessions than those possible using traditional analysis approaches. Again, operating in data space will leverage DOI analysis to answer many questions that traditional analysis approaches cannot. 

As part of this dissertation, we seek to create the foundation for DOI eye-tracking analysis (i.e. DOI analysis) data. Our research goals will be achieved by the two contributions below:

\textbf{Contribution-1:} We aim to show that collecting sufficiently accurate DOI analysis data is feasible. In a DOI eye-tracking experiment, experimenters will instrument their visualization's source-code in order to produce DOI data. Generally, DOI data will be a time-annotated collection of objects viewed by the participating users in such experiments. 

Intuitively, we can define viewed objects as those objects that gaze samples fall in, a naïve method already used in AOI analyses. Since AOIs are usually large and non-overlapping, this method is adequate in such situations. However, the real-life visualizations that we aim to track can be complicated and dense. For example, hundreds of distinct visual objects may occupy the screen at the same time. Rather than pinpointing precise pixel coordinates, eye-trackers can only indicate small screen regions (e.g. approximately one inch in diameter), which users are fixating (i.e., viewing). Since such regions are highly probable to intersect multiple visual objects, mapping gazes to individual objects is bound to be an imprecise process. Thus, we aim to investigate if DOI instrumentations can produce data that is sufficiently accurate for meaningful analyses in the context of real-life visualizations.      

As such, we seek to improve the naïve AOI detection approach. We aim to do so based on the hypothesis that users are more likely to view objects that are visually appealing (e.g. highlighted), or connected (physically or semantically) to previously viewed objects. If this were true, it would allow us to distinguish between potentially viewed objects, when a gaze is detected in the vicinity of multiple objects. We aim to test this hypothesis, formalize this idea into a novel DOI detection algorithm, and evaluate its performance over the naïve AOI detection approach.

To pursue this contribution, we will incrementally develop a DOI collection algorithm. We will instrument and apply this algorithm to collect DOI data from users solving real tasks in a real-life visualization. Afterward, we will evaluate the reliability and effectiveness of the collected DOI data. We discuss a tentative plan regarding this contribution in Section~\ref{sec:Contrib1}.

\textbf{Contribution-2:} We aim to formalize the process of collecting and interpreting DOI data, determine the types of questions that DOI data could answer, and create novel visual analytics support to answer them. These steps will allow researchers to harness the maximum potential of DOI analysis. 

DOIs are closely related to AOIs. Moreover, using AOIs to analyze eye-tracking data is well understood and a plethora of visualization techniques exist to support such analyses. However, we claim that using these established AOI analysis methods to understand DOI data will not be effective. First, our preliminary work shows that DOI data is significantly more granular and larger than data collected in traditional eye-tracking experiments. For example, using the DOI approach, we could track hundreds or thousands of DOIs over hour long experimental sessions. This contrasts with traditional AOI methods which typically track tens of AOIs over one or two minutes. AOI methods are unlikely to handle the significantly larger volumes of DOI data. Second, DOI data can be more useful in getting insights about the semantics of the data a user explores, since DOIs are associated with data attributes. Such attributes are generally unavailable in AOI data, and AOI methods have not been designed to explore them. Thus, AOI methods will not be sufficiently flexible to answer the novel questions that DOIs can answer. 

We divide this contribution into three sub-contributions: formalizing DOI data collection, formalizing the types of analysis questions that can be asked to DOI data, and designing visual solutions to facilitate the analysis of DOI data.

\textbf{Sub-contribution 2.1:} We aim to \textbf{formalize the DOI data collection} process by providing guidelines to experimenters about how to instrument visualizations and collect DOI data. We claim that DOI data can be difficult to analyze if data is collected in a clumsy format. We aim to provide a data model of DOI to enable experimenters producing adequately formatted DOI data. For example, in a network visualization, DOIs may be individual nodes or clusters of nodes. Generally, links represent a single relationship among nodes. Thus, simple links are unable to represent multiple semantic relationships among DOIs. Hence, a representation of all necessary relationships among DOIs to test intricate hypotheses afterward. On the other hand, testing hypotheses may become infeasible if DOI data is collected without any data model. Thus, we need a DOI data model to facilitate experimenters.

\textbf{Sub-contribution 2.2:} Generally, analyzers test their hypotheses by questioning their experimental data. We aim to \textbf{formalize the type and range of analysis questions that can be asked of DOI data}. This will allow researchers to understand the types of scientific questions that the DOI methodology can support. Methodologically, we plan to start from the formal data model devised as part of contribution 2.1 and exhaustively identify the types of questions that that data model can support, an approach used with good results in the past to generate task-requirements for other types of data (e.g., geographical data, temporal data). 

\textbf{Sub-contribution 2.3:} Presently, visualization is an essential tool for data analysis. We aim to \textbf{explore designs of visual solutions that could facilitate the analysis of DOI data}. We will explore existing visual techniques for analyzing AOI data, and implement new visualizations based on them to support the interpretation of the larger and richer DOI data. We also aim to turn these designs into a functional DOI analysis-system. We will explore these methods and their effectiveness while trying to help real-life researchers answer real-life scientific questions. Specifically, we will ask for design requirements and feedback from collaborators at FIU and plan to incrementally modify our designs according to their suggestions.

More details regarding this contribution are provided in Section~\ref{sec:Contrib2}.

 