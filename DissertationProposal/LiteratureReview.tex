\chapter{Literature Review}
Eye-tracking technology is gradually getting more accurate, faster and cheaper~\cite{Duch07}. As a result, it has been a popular tool in many research fields such as psychology, neuroscience, human-computer interaction (HCI), and data visualization~\cite{Duch02}. Specifically, Duchowski categorized eye-tracking applications into two classes: interactive and diagnostic~\cite{Duch02}.

Interactive eye-tracking has been used as an alternative to pointing devices (e.g. mouse, touch-interface) in 2D~\cite{Jacob91} and 3D~\cite{Bolt90, Tan00}, and even as a text-input device \cite{Maj02}. However, eye-tracking generally proved ineffective compared to traditional selective devices (e.g. mouse, touch, and keyboard) due to the difficulty to differentiate between view-gazes and interaction-gazes. This is known as the ``The Midas Touch Problem''~\cite{Jacob91}.

The proposed research in this dissertation primarily focuses on eye-tracking's diagnostic role. The diagnostic use of eye-tracking is often involved in eye-tracking user studies. The most common form of a diagnostic eye-tracking study is a user solving visual tasks by observing visual stimuli on a computer screen while an eye-tracker records the user's gaze positions. Then, analyzers process the gaze data offline to understand how people observed the stimuli and solved the tasks~\cite{Duch07}. In this way, eye-tracking was used to understand how people recognize faces~\cite{Guo14, Sha14}, how attention changes with emotion~\cite{Ver13}, how diseases may affect perception~\cite{Kim14}, and how student learn from visual contents~\cite{Zaw15, May10, vGo10, Con13}.   

The use of eye-tracking in data visualization research has increased with the growing popularity, accuracy, and affordability of the technology. For example, major contributions building on eye-tracking technology include the network readability study by Pohl et al.~\cite{Poh09} and Huang et al.~\cite{Hua08, Hua05}, the study on tree drawing perception by Burch et al.~\cite{Bur11, Bur13}, and that on decision-making visualization by Kim et al.~\cite{Kim12}. 

Research studies using diagnostic eye-tracking heavily depend on analyzing eye-tracking data. The analysis methods for eye-tracking data can be divided into two paradigms: point-based methods and area of interest (AOI)-based methods~\cite{Bla14}. In a point-based analysis, each of the gaze samples or fixations is treated as a discrete point. As such, point-based analyses generally report overall fixation patterns as spatial or temporal distributions of 2D gaze coordinates over visual stimuli. The major weakness of this approach includes the requirement to show the same set of stimuli to the human subjects in order to be comparable, and the requirement to always analyze gaze-data in conjunction with the 2D screen capture for which it was collected. Analyzing each stimulus separately will result in longer analysis time for large numbers of stimuli. Moreover, analyzers have to manually relate gazes with the semantic contents of stimuli. This approach is highly ineffective in the case of interactive and dynamic stimuli. 

Alternatively, analyzers often define AOIs that are important to their hypotheses onto a stimulus~\cite{Bla14}. The number of gazes landing into an AOI can then be computed automatically as a proxy for users' interest in that AOI. Higher level analyses are thus possible. Examples include but are not limited to investigating reading patterns where each word is an AOI~\cite{Bey05, SR08}, to observe where and how long users look at visual regions~\cite{Coco09, Kim12}, and to compare interfaces utilizing AOI fixation counts and frequencies~\cite{Coletkin09}. Usually, defining AOIs over stimuli is done manually and is generally very time consuming. Thus, the analysis process takes longer time with the increase count of stimuli and visual contents within the stimuli. Moreover, the process becomes prohibitively inefficient for interactive and dynamic stimuli (e.g. video) since AOIs need to be defined for each frame of a video. 

Several solutions are proposed to overcome this weakness. One example is the automatic AOI annotations using gaze clustering algorithms~\cite{Pri00, San04, Dru14}. However, an increase of complexity of visual contents in stimuli may increase the difficulty of the AOI annotation process. Stellmach et al. proposed the object of interest (OOI) concept for 3D stimuli where gazes are collected on the surface of 3D objects available in a scene~\cite{Ste10}. Additionally, Steichen et al.~\cite{Ste13} and Kurzhals et al.~\cite{Kur14} suggested the possibilities of dynamic AOI annotation in case of computer generated visual contents. However, this concept is still unexplored. The proposed research leverages this concept of dynamic AOI and OOI into developing DOIs. 

The idea of DOI originates from the objective of detecting automatically which data-objects a user of a visualization views. As such, DOI is the mapping of gaze samples to data objects rather than pixel positions. Recently, Sundstedt et al.~\cite{Sun13} and Bernhard et al.~\cite{Bern14} introduced a process called gaze to object mapping (GTOM) for identifying objects which are targets of users' attentions in 3D virtual environments. We also proposed a similar approach to in the context of relating gazes with semantic contents of network diagrams~\cite{Okoe14}. However, relating gazes with semantic contents of any visualization is non-existent. Salvucci et al. presented a probabilistic approach to predict viewed objects on a computer screen using eye-tracking~\cite{Sal00}. Moreover, predictions of which objects are viewed can be significantly improved by leveraging semantics of visual contents as indicated by Salvucci et al. alongside with Okoe et al.~\cite{Okoe14}. However, both Salvucci et al.'s and Okoe et al.'s contributions were limited to simple visualizations. Salvucci et al.'s method was tested over a simple gaze-added WIMP (Window, Icon, Menu, and Pointer) interface, while our own work was limited to network visualization. Our first contribution is inspired by the idea of OOI and GTOM. However, our methods will be significantly different and will seek to be compatible for more complex visualizations. As we will show in Chapter~\ref{chap:ResearchPlan}, one of our methods intends to innovate on existing techniques for mapping gazes to objects by adopting the probabilistic method by leveraging semantic contents of visualizations. 

Our contributions also focus on the interpretation of eye-tracking data. Many methods exist to visually interpret eye-tracking data.  Blascheck et al. categorized several existing visualization techniques for point-based and AOI-based visualizations~\cite{Bla14}. More complex visual analytics software systems and solutions include those by Andrienko et al.~\cite{And12}, Weibel et al.~\cite{Wei12}, Kurzhals et al.~\cite{Kur14}, and Blascheck et al.~\cite{Bla16}.  

However, DOI data can be significantly more granular and larger than AOI data. Moreover, DOIs can be associated with a wealth of data attributes derived directly from the tracked data. Hence, we hypothesize that DOI data can answer questions that AOI cannot, and that data interpretation using traditional AOI analysis methods are ineffective for DOI data. This motivates our second contribution, with its two sub-goals: determining what questions DOI data can answer that AOI cannot, and creating support for these questions. 

To accomplish the former, we aim to formalize the analytical tasks that can be pursued via DOIs. A specific categorization of analysis tasks for eye-tracking data is currently non-existent. However, task categorizations, task taxonomies, and task frameworks do exist for other types of data and analyses. For example, Wehrend and Lewis, and Shneiderman~\cite{Weh90, Shne96} discuss general features of task taxonomies in the context of data visualization. Recently, Brehmer et al., Schulz et al., and Rind et al. proposed a multilevel typology that can be applicable for creating complete task descriptions regardless of domain specifications~\cite{Bre13, Sch13, Rind15}.  Amar et al. provided a comprehensive categorization of low level tasks~\cite{Ama05}.  Moreover, task taxonomies are proposed for specific types of data visualizations such as for graph visualization~\cite{Lee06}, group level graphs~\cite{Sak14}, multidimensional data visualization~\cite{Ward02}, and geo-temporal data~\cite{And03, Roth13}. Hence as a part of our second contribution, we draw inspiration from these studies for an attempt to categorize DOI analysis tasks. 

To achieve the latter part of our second contribution, we aim to explore designs of visualization support to perform analysis tasks for DOI, and employ existing visual support available for AOI analyses. We will also attempt to adopt interaction techniques from Yi et al.'s interaction taxonomy for information visualization~\cite{Yi07} to support DOI analysis tasks. 