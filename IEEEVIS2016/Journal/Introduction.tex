\section{Introduction}
Eye-tracking is invaluable at explaining how people perceive, solve visual tasks, and use interfaces ~\cite{duchowski2002breadth}. However, much of the technology's full potential is untapped, as most eye-tracking studies explore key-hole scenarios involving static images, simple tasks, and no interaction with visual content. This is at least in part because gazes are traditionally analyzed as pixel coordinates of evaluated visual frames. Relating these coordinates back to the semantic content of each frame viewed in an experiment (i.e., determining whether a gaze matches an object in the image) involves extensive manual input, and is prohibitively time-consuming for complex experiments.
 
Recently there is increasing interest in analyzing directly what users look at rather than where they look~\cite{alam15analyzing, sundstedt2013visual, bernhard2014gaze}. Such methods are applicable to visual content that is computer generated. Since positions and shapes of objects shown on the screen are known during rendering, gaze coordinates can be mapped automatically and in real time to a visualization's semantic content.  

Such methods output sequences of data-objects that subjects view during an experiment (e.g., nodes in a network, 3D objects in a scene). We will refer to these objects as data-of-interest (DOI), a name used by Alam et al. to underline the analogy with areas of interest (AOI)~\cite{alam15analyzing}. DOIs are subsets of the data underlying an experiment, and are thus described by attributes that are derived from that data (e.g., labels, types of objects, quantitative attributes). Moreover, DOI methods capture subjects' interest in DOIs irrespective of the visual context in which these are observed. For example, in a node-link diagram, a user's interest in a node will be tracked regardless of whether that node moves as a result of interactions, or whether the visualization as a whole is zoomed or panned. This means that once a visualization is instrumented, data can be captured without additional effort from interactive systems over extended periods of time~\cite{alam15analyzing}. Moreover, as long as subjects explore the same data-space, their results will be comparable in that space, even if they do not see the same rendered views. 

These properties mean that DOI methods can support research workflows not previously possible~\cite{alam15analyzing, sundstedt2013visual, bernhard2014gaze}. Our paper makes two necessary contributions on the way to applying DOI methods in practice.
 
First, we formalize the range of questions that can be asked of DOI data by introducing a DOI task taxonomy. Task-taxonomies help drive the development of novel visual encodings and visual analytics designs by formalizing requirements and supporting evaluation~\cite{amar2005low,okoe2015graphunit}. However, no task taxonomies exist for DOI data, or even for more established AOI analyses. We will show that this contribution is significant as DOI data can support tasks and workflows different from those that AOI data can.

Second, we define and discuss the design space of visual encodings and interactions that can support DOI workflows. New visual designs are necessary because existing methods for AOI analysis do not scale to DOI data, and do not support the types of questions that DOI data can answer. To define this design space, we start from existing visual methods for AOI analysis, and explore ways in which they can be adapted to support DOI analyses. We structure our discussion around Yi et al.'s~\cite{yi2007toward} taxonomy of visualization interactions, and consider solutions that can support the tasks described in our DOI task taxonomy. This analysis represents a baseline for future design studies and systems that will explore the use of DOI analyses in practice.


