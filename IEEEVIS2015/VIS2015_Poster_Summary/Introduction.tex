\section{Introduction}
\label{sec:Introduction}

Eye-trackers tell us where on the computer screen users are looking, and have been a valuable diagnostic tool in disciplines such as psychology, cognitive science, human-computer interaction, and visualization research~\cite{duchowski2002breadth}. Traditional eye-tracking workflows (e.g., point-based analysis, area of interest (AOI) analysis~\cite{blascheckstate}), rely on gaze coordinates collected in conjunction with rendered visual stimuli, and require a significant amount of human annotation to relate gazes to the semantic meaning of the stimuli~\cite{alamdata}. This meant  that eye-tracking analyses were generally performed laboriously, offline.  

A new breed of accessible eye-trackers (e.g., around $150$) opens up the possibility of equipping regular workstations with eye-trackers, enabling novel eye-tracking applications. One is the monitoring of eye-tracking data in real-time. For example, it is conceivable that classroom computers could be equipped with eye-trackers, and visual learning environments instrumented to capture what students are looking at. Instructors could use such data in real time to identify students who struggle and provide proactive help. Similarly, if regular workstations featured eye-trackers, visual analytics systems could be instrumented to monitor user's data interests and make recommendations. Finally, commercial add-placement systems could benefit from integrating real-time eye-tracking information with manual interactions to improve their recommendations.

Such real-time analyses are facilitated by  Alam et al.'s recent proposal of data of interest (DOI) eye-tracking data interpretation~\cite{alamdata}, which involves instrumenting the rendering code of a visualization to automatically relate gazes to the visual content displayed on the screen. The DOI method outputs in real-time what data-objects users are looking at during a visual exploration. Moreover, because this data is the same as the one underlying the instrumented visualization, it has direct semantic meaning, is tied to the tasks users are doing, and can be useful even if separated from the visual stimulus from which it was collected. 

We hypothesized that an analyst could look at such data in real-time, as it is streamed from multiple users of a visualization concurrently, to infer what those users were doing. We validated this hypothesis through a user study.   
 

 
