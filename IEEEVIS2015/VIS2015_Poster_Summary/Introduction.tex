\section{Introduction}

Eye-trackers can tell us where on the screen computer-users are looking, and have been used extensively as diagnostic tools in disciplines such as psychology, cognitive science, human-computer interaction, and Visualization research~\cite{duchowski2002breadth}. Traditional eye-tracking workflows, such as point-based analysis and area of interest (AOI) analysis~\cite{blascheckstate}, rely on gaze coordinates collected in conjunction with rendered visual stimuli, and require a significant amount of manual input from human annotators who have to relate gazes to the semantic meaning of the stimuli~\cite{alamdata}. This meant  that eye-tracking data was generally performed relatively laboriously, offline.  

This may have been acceptable in the past as eye-trackers tended to be expensive and data relatively difficult to collect. However, a new breed of accesible eye-trackers (e.g., around $150$) opens up the possibility of equipping regular workstations with eye-trackers, leading in turn to novel applications of this technology. One of these is the analysis of eye-tracking data in real-time. For example, it is nowadays conceivable that classroom computers could be equipped with eye-trackers, and visual learning environments instrumented to capture in detail what students are looking at. Such data could be used in real time by instructors, to identify students that don't tend to concepts known to be important to learning, and provide proactive help. Similarly, regular workstations could feature eye-trackers, and visual analytics systems could be instrumented to keep track of what data and tasks users are performing in order to suggest recommendations. Finally, commercial add-placement systems could benefit from integrating real-time eye-tracking information with manual interactions to improve recommendations.

Such real-time analyses can be facilitated by  Alam et al.'s recently proposal of data of interest (DOI) interpretation of eye-tracking data~\cite{alamdata}. This approach simplifies the collection and analysis of eye-tracking data, and involves instrumenting the rendering code of a visualization so that gazes  are automatically related to the visual content that is displayed on the screen. The DOI method can thus identify in real-time what data-objects users are looking at during their visual exploration. Moreover, because this data is the same as the one underlying the instrumented visualization, it has direct semantic meaning, is tied to the tasks users are doing, and can be useful even without seeing the visual stimulus from which it was collected.

We hypothesized that an analyst could look at such data in real-time, as it is streamed from multiple users of a visualization concurrently, and infer from it what those users are doing. We validate this hypothesis through a user study described below.   

 
