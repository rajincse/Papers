\section{Discussion}
\label{sec:Discussion}
\subsection{Benefits}
\label{sec:Benefits}

It took each of our coders approximately six hours to produce a detailed and accurate coding of just eighteen minutes of user data ($6$ subjects $\times$ $3$ minutes). This illustrates the importance of moving beyond interpreting eye-tracking in stimulus space. The instrumentation we described and evaluated makes it feasible to analyze eye-tracking data from many subjects, using highly interactive content, for long analysis sessions. Such analyses can be done immediately after or even as the data is collected since no manual annotation of the data is required. 
Moreover, the collected data has semantic meaning that is tied to the underlying data of the visualization and is thus amenable to a much richer set of visual and computational analyses than traditional eye-tracking data. Such analyses, which we exemplified in Sections~\ref{sec:EvalDataCollected} and~\ref{sec:EvalAssumptionAboutViewingTransition} , focus on data and concepts, and are thus significantly different from current eye-tracking analyses which generally aim to understand low-level visual perception in static visualizations.

Our work provides a currently lacking quantitative framework that can be used to explore questions related to how users perceive visualizations in general, how domain experts look at particular types of data, and how analysts use visualization to search for relevant data and aggregate them into hypotheses. Visualizations researchers often have to rely solely on discussions with domain experts, think aloud studies, and recordings of user activity, all of which provide only qualitative data and often require additional coding and interpretation.


\subsection{Applications}
\label{sec:Applications}
As described above, our approach can be useful in understanding how users forage for, integrate, and hypothesize about data using complex, domain specific, visualization systems. For example, visual analytics applications could be instrumented to facilitate the exploration of domain expert workflows, of how expertise influences data search and analysis patterns, and of visual strategies and data associated with successful hypothesis generation and testing. Similarly, the instrumentation of visual learning environments could lead to insight into how students learn and what makes some learners more effective than others. Given the proliferation of education through visual, interactive environments, particularly as part of massive MOOC instruction, this could have significant impact. 

In both aforementioned cases, the data can also explain how visualizations support analysis, discovery, and learning, and how they may be changed to make them more efficient. For example, given a particular domain, we could quantify which data best answers which questions, what types of data are often used together, and how visual widgets are viewed in an analysis process. We could then use such data to optimize specific visualizations systems or generic visualization methods.  Quantitative and qualitative analyses such as those described in Section~\ref{sec:Evaluation} could be used to this end.  

More differently, viewing data collected automatically during a user's session could be used more directly to support analytic workflows. For example, the data could be transformed automatically into summaries that capture the user's activity during a day, week, or month. Such summaries could be used to refresh the user's memory at a later time, communicate progress to peers or supervisors, and provide useful hints to other users or analysts exploring similar questions in similar data-sets.

Detecting viewed objects online opens up two specific opportunities. First, analyzing eye-tracking data in real-time could be used in teaching. By instrumenting learning environments, we could allow instructors to track students' progress in a lab assignment in real-time, to detect students that are not tending to elements crucial for solving or understanding the assigned problems, and to provide help proactively. Second, it would allow us to create a new generation of gaze-contingent visualizations that can detect in real-time data that is of particular interest to a user and make recommendations of unexplored data with similar attributes. The ever lower cost of eye-trackers, currently around $200$, makes these research avenues interesting and potentially impactful. 


\subsection{Limitations}
\label{sec:Limitations}
Our approach is restricted to visualizations with open source code and cannot be used to automate the full spectrum of current eye-tracking studies (e.g., analysis of real imagery or of commercial systems).   This problem is to some degree inherent to any software or hardware instrumentation: whether one wishes to capture an application's interaction data, a website's activity, or a network's throughput, one needs privileged access to those systems. Thus, like most instrumentations, our approach is intended primarily for creators or owners of data visualizations who wish to understand how their users use their visualizations, and to discover changes that could make their visualizations more efficient. Moreover, our approach creates additional analysis and interaction opportunities, a few of which we exemplified in Section~\ref{sec:Methods}. 

Second, instrumenting a visualization by altering its source code and defining transition and viewing probabilities involves an overhead. This is also a general instrumentation problem and is solved by a case by case consideration of the tradeoff between the overhead of instrumenting a specific system and the benefits of collecting data from it. For example, if the development of a visual analytics system takes a year from requirements elicitation to final implementation and instrumenting it would allow developers to gain significant insight into how the system is used, then spending even an extra week to mirror the rendering code with an instrumentation library may seem warranted.  Our future plans include bundling the predictive algorithm an instrumentation library (Section~\ref{sec:FutureWork}). This would reduce the cost of instrumenting a visualization significantly. 



Finally, picking the right parameters to our predictive algorithm may be difficult for some visualizations, given that a solid understanding of how users parse and interpret visualizations does not yet exist. To address this, in Section~\ref{sec:EvalAssumptionAboutViewingTransition}  we give a methodology to quantify transition and viewing probabilities from real data collected from users. Such computations could be performed during a pilot studying to reveal usage patterns in a particular visualization. Furthermore, since we believe visualizations are rarely used in a random fashion and that specific tasks and visual outputs elicit certain gaze patterns, we think further research could lead to a more general understanding of the probabilities our algorithm relies on.  Moreover, our framework facilitates exactly this type of research in ways previously not possible, as exemplified by the analyses in Section~\ref{sec:Evaluation}. We discuss our plans in Section~\ref{sec:FutureWork}. 
 

\subsection{Performance gains by using a probabilistic approach}
An important contribution was to show that by leveraging a predictive model of how users view data in a visualization we can detect objects more accurately than by just relating gazes to visual object geometry. While in our particular example the gain was relatively small ($5\%$), we think that benefits are highly dependent on the type of visualizations that are instrumented, and that some visualizations will benefit significantly more from the predictive approach. 

Table~\ref{tab:TransitionFromMovie} suggests that there can be very strong biases in how people use visualizations (e.g., subjects were between $3$ and $11$ times  more likely to view a highlighted item connected to a previously viewed item, than they were to view a random item). We believe this to be generalizable to many visualizations, especially those that show large, heterogeneous data, and those that are intended for in-depth, focused analyses. The first aspect means that the same data are likely to be used differently based on context and task. The second aspect means that  tasks can significantly constrain what data is viewed, as shown in Section~\ref{sec:Limitations} ,   

The degree to which such viewing patterns can and need to be leveraged predictively depends on the visual particularities of visualizations. For example, in our particular case study the different data categories (i.e., movies, directors, actors, genres) were spatially separated in different panels. As such, if a gaze landed between multiple data objects, these were generally of the same type. This means that our algorithm never got the chance to use object category as a discriminator. Instead, in a traditional node link diagram for example, multiple definable categories of nodes share the same space, and are distinguishable by specific visual attributes or semantic meaning (e.g., proteins in a protein interaction network can be kinases, receptors, etc.). In such a case, an algorithm could use knowledge that a user is currently scanning for, or generally more interested in, a particular type of node, to distinguish between the viewing of nodes that are placed next to each other but are from different categories.  

More generally, a visualization will benefit more from our predictive approach if heterogeneous content is cluttered and shares the same space, and the visualization provides visual and semantic cues that allow users to select subsets of  data that are relevant to a particular task or analysis. Such visualizations are fairly commonly used in real, domain specific visualization applications. Instead, if the visual content is sparse and well separated, then computing gaze scores alone would be sufficient and our algorithm's predictive component would not create any benefit.



\subsection{Evaluating viewed object detection} 
\label{sec:DiscussionEvaluatingViewedObjectDetection}

The above mentioned variability in accuracy makes it hard to assess the real impact of the predictive method. Moreover, comparing the output of the predictive algorithm to annotations of human coders is questionable since, if coders look primarily at momentary gaze positions, rather than trying to understand what users aim to do more broadly, then their annotation may be closer to our our simpler, probabilistic detection.  This latter problem raises an issue about whether human coders can provide a robust ground truth for evaluating techniques such as ours, and whether such ground truths may be improved if eye-tracking data was collected in conjunction with a think-aloud protocol.  

First, we note that we see the quantitative evaluation described in Section~\ref{sec:Evaluation} as an evaluation against the state-of-the –art rather than against a ground truth. In other words, we don't claim that our method produces results that are accurate with respect to what people actually looked at. Instead, we claim that our method allows us to analyze the data in the same way a human could, only much faster. 

Second, we believe that striving towards a reliable ground truth is slightly misguided in the context of evaluating eye-tracking instrumentation.  People often view elements even without consciously realizing it, since vision is by-and-large a subconscious process~\cite{duchowski2007eye}. People also are able to register multiple objects in a fixated region, while not fixating any one object specifically. For example, while reading people often skip short words or syllables, while still registering that they are there. Moreover, for specific tasks, people may think about multiple objects as single data units of analysis. For example, a user of a graph visualization might think in terms of nodes for some tasks (e.g., are two nodes connected?) but may reason in terms of clusters of nodes or cliques for other tasks (e.g., how many clusters of nodes are in the graph?). In the latter case users may fixate at the center of a node cluster to assess the properties of the cluster as whole, rather than analyze individual nodes. Finally, people also occasionally stare at visual objects while in fact thinking of something else~\cite{duchowski2007eye}. These issues lead to interesting questions about whether we track what subjects look at or what they see.  


As such, we believe a clean ground truth that represents what a subject actually looked at is either unattainable or, if obtained through think allowed protocols or highly constrained tasks, would not be representative of real-life usage scenarios. Specifically, we hypothesize that should experimenters ask subjects to state what they are looking at, or look at particular objects, this would change not only what items subjects look at, but also how they look at them in terms of low level gaze patterns (e.g., subjects may tend to fixate closer to or directly on an object). Such artifacts are known to occur when using think-aloud protocols, and we think they would be even more prevalent due to the subconscious, intuitive, and fast nature of visual perception.  Evidence for this is given by Ogolla who showed that current think-aloud protocols changes visual patterns, especially for exploratory tasks~\cite{ogolla2011usability}.
 

Finally, the evaluation described in Section~\ref{sec:EvalDataCollected} implements in fact an evaluation against a ground truth that is loosely defined by the tasks subjects had to do. These tasks, especially the structured ones, dictated what users had to look at in order to solve them, and the two visual representations in Figures~\ref{fig:heatmap} and Figure~\ref{fig:RelevanceDiagram}, indicate how close our instrumentation method comes to that ground truth. Such ground truths are somewhat approximate and not sufficiently detailed, but can nevertheless show that data collected automatically is relevant. 


\subsection{Future Work}
\label{sec:FutureWork}
We hypothesize that there is a set of general principles about how people take in visual and data content that are valid across visualizations. For example, most visualizations have a mechanism for highlighting specific elements, either through interaction or through queries, and we show that this highlighting matters in how people view elements. Second, while we studied connected elements in the context of a node-link like diagram, establishing a visual connection between elements is also employed in brushing and linking interactions or the use of leader lines. We think our finding that users view connected elements together also applies to these more general cases. Third, most data featured in visualization can be divided into semantic groups (e.g., actors, movies, directors; protein kinases, protein receptors; conference papers, journal papers) and we hypothesize that viewing transitions between and within such categories are also not random. Finally, we showed that in our particular case study, users identified data that is highly connected to their task and then shifted their attention repeatedly and almost exclusively within that data group. Again, we believe this is a behavior that is generalizable. Demonstrating these generalities and exploring other patterns is beyond the scope of this paper but the framework we proposed allow us to easily explore and quantify such patterns in other visualizations. This would both deepen our understanding of how visualizations are used and provide guidelines for choosing appropriate inputs to our algorithms. 

More work is also needed to understand the impact of different parameters involved in viewed object detection. For example, how far away from an item can a user fixate and still be considered to be viewing the item? The parameter that captures this in our algorithm is $R$, and, while we use a constant $R$ for all items, this is unlikely the best approach. Based on qualitative analyses the data we collected,  and knowledge of the interplay between peripheral vision and the fovea~\cite{balas2009summary}, we believe users fixate close to items if they are surrounded by clutter,  but exhibit significantly more variability if items are isolated. Thus, we hypothesize that $R$ should adjust itself dynamically based on the clutter of the region that a user is fixating.  A further question is whether $R$ should be changed based on the visibility or discriminating features of an item: can subjects fixate farther and still perceive an item, if that item is large enough?

Additional work is also needed to understand how to and whether we can detect visual objects other than nodes or labels, such as for instance polylines in a parallel coordinate plot, contours in a group or set visualization, or cells in a heatmap. It is unclear how to compute a gaze score ($gs$) for such objects since there is no research to describe how people fixate them. 

Finally, to reduce the overhead of instrumentation, our future plans include making the predictive algorithm available as an instrumentation library. A developer would link this library to the visualization and maintain a correspondence between what is shown on the screen at any given time and visual objects registered with the instrumentation library. Thus, when a new object is added to or removed from the screen, or when its position or shape changed, these changes would need to be registered with the library. Interestingly, this workflow integrates well with the add-remove-update pattern typical of D3. Additionally, developers would create classes of objects, for instance based on data semantics (e.g., kinases, movies, actors) or visual aspect (e.g., highlighted, glyphs of a certain kind), and specify transitions probabilities between them. The library would implement the algorithms described here and provide in real time a list of visual items that a user is viewing.
  