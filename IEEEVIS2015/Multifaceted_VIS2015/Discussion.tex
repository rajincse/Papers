\section{Discussion}
\label{sec:Discussion}
\subsection{Benefits and opportunities of analyzing eye-tracking data in visualization space}

It took each of our coders approximately six hours to produce a detailed and accurate coding of just twelve minutes of user data ($6$ subjects $\times$ $2$ minutes). This illustrates the importance of moving beyond interpreting eye-tracking in stimulus space.

The instrumentation we described and evaluated makes it feasible to analyze eye-tracking data from many subjects, using highly interactive content, for long analysis sessions. Such analyses can be done immediately after or even as the data is collected since no manual annotation of the data is required. Moreover, the collected data has semantic meaning that is tied to the underlying data of the visualization and is thus amenable to a much richer set of visual and computational analyses than traditional eye-tracking data. Our evaluation exemplifies this with the sorting of heatmaps based on the viewed item' type, and our ability to compute a relevance measure on viewed items. 

As such eye-tracking in visualization and data space could be used to understand how users forage for, integrate, and hypothesize about data using complex, domain specific, visual analytics systems. This would be significantly different from current eye-tracking analyses which generally aim to understand visual perception and low level visual strategies in static visualizations, and could play an important role in advancing the visual analytics agenda. 

\subsection{Improving viewed object prediction and detection} 

In Section~\ref{sec:Methods} we describe two ways to estimate viewing transition probabilities between items in a visualization: an intuitive way and a principled way. A third way can emerge from more research on this subject. Specifically, we hypothesize that there is a set of general principles about how people take in visual and data content that are valid across visualizations and can be used for the purpose of improving viewed object detection. Such principles include but are not limited to how people see highlighted items, how people revisit items, how visual properties of an item (e.g., size, label, details) may influence how it is perceived, and what the interplay between manual interactions and visual interactions is. Automating eye-tracking workflows would allow us to collect large amounts of data and answer such questions, which in turn could be used to improve our ability to detect visual objects accurately.  


More work is also needed to understand the impact of different parameters involved in viewed object detection. For example, how far away from an item can a user fixate and still be considered to be viewing the item? The parameter that captures this in our algorithm is $R$, and, while we use a constant $R$ for all items, this is unlikely to be an optimal approach. Based on qualitative observations of our collected data and knowledge of the interplay between peripheral vision and the fovea~\cite{balas2009summary}, we believe users fixate close to items if they are surrounded by clutter,  but exhibit significantly more variability if items are isolated. Thus, we hypothesize that $R$ should adjust itself dynamically based on the clutter of the region that a user is fixating.  A further question is whether $R$ should be changed based on the visibility or discriminating features of an item: can subjects fixate farther and still perceive an item, if that items is large enough? Current models of peripheral vision state that clutter is a more determining factor in peoples’ ability to use their peripheral vision~\cite{balas2009summary}, but further research is required to quantify the effects of such algorithmic instrumentation choices. 




Finally, additional work is needed to understand how to and whether we can detect visual objects other than nodes or labels, such as for instance polylines in a parallel coordinate plot, contours in a group or set visualization, or cells in a heatmap. It is unclear how to compute a gaze score ($gs$) for such objects since there is no research to describe how people fixate them. 

\subsection{Evaluating viewed object detection} 

Evaluating the real impact of 'intelligent ' gaze detection is hard as it depends on many factors. For example, if the visual content is sparse, then computing gaze scores alone would be sufficient and our algorithm's predictive component would not create any benefit. The content of the visualization can also be more or less amenable to prediction. For example, we hypothesize that employing prediction in a parallel coordinates plot is much harder than doing it with well separated, point-like objects, such as nodes in a network. Even comparing our results to human coders is questionable in terms of evaluating our prediction algorithm since, if coders look primarily at momentary gaze positions, rather than trying to understand what users are trying to do overall, then their annotation may be closer to our raw gaze scores rather than scores that integrates prediction. 


This also raises an issue about whether human coders can provide a robust ground truth for evaluating techniques such as ours, and whether such ground truths may be improved if eye-tracking data was collected in conjunction with a think-aloud protocol. 


First, we note that we see the quantitative evaluation described in section~\ref{sec:Evaluation} as an evaluation against the state-of-the –art rather than against a ground truth. In other words, we don't claim that our method produces results that are accurate with respect to what people actually looked at. Instead, we claim that our method allows us to analyze the data in the same way a human could, only much faster. 

Second, we believe that striving towards a reliable ground truth is slightly misguided in the context of evaluating eye-tracking instrumentation.  People often view elements even without consciously realizing it, since vision is by-and-large a subconscious process~\cite{duchowski2007eye}. People also are able to register multiple objects in fixated region, even though not fixating them specifically. For example, while reading people often skip short words or syllables, while still registering the fact that they are there. This in fact leads to an interesting question about whether we track what a subject looks at or what a person sees. Finally, people also occasionally stare at visual objects while in fact thinking of something else~\cite{duchowski2007eye}. 


As such, we believe a clean ground truth that represents what a subject actually looked at is either unattainable or, if obtained through think allowed protocols, would not be representative of real-life usage scenario. Specifically, we hypothesize that should experimenters ask subjects to state what they are looking at, or look at particular objects, this would change not only what items subjects look at, what also how they look at them in terms of low level gaze patterns (e.g., subjects may tend to fixate closer to or directly on an object). Such artifacts are known to occur when using think-aloud protocols, and we think they would be even more prevalent due to the subconscious, intuitive, and fast nature of visual perception.   

Finally, we believe our second type of evaluation, described in section~\ref{sec:EvalDataCollected}, is in fact an evaluation against a higher level ground truth. Our tasks, especially the structured ones, dictate what users will have to look at in order to solve the tasks, and the two visual representations in Figures X and Y, indicate how close our instrumentation method comes to that ground truth. While these ground truths are somewhat approximate and not sufficiently detailed to capture the small differences between instrumentation algorithms, we believe they are well suited at evaluating approaches such as ours.   
