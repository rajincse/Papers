\section{Discussion}

\textbf{Benefits and opportunities of analyzing eye-tracking data in visualization space:} It took each of our coders approximately two hours to produce a detailed and accurate coding of just four minutes of user data. AOI analysis would not have helped in the task we asked them to do since views contained many small visual objects and users zoomed, panned, and changed views often. This illustrates the important of moving beyond interpreting eye-tracking in stimulus space.

The instrumentation we described and evaluated makes it feasible to analyze eye-tracking data from many subjects, using highly interactive content, for long analysis session. Such analyses can be done immediately after or even as the data is collected since no manual annotation of the data is required. Moreover, the collected data has semantic meaning that is tied to the underlying data of the visualization and is thus amenable to a much richer set of visual and computational analyses than traditional eye-tracking data which is only defined by time and position. A very basic example is our sorting of the heatmap based on the viewed objects' type, but more complex analyses could be application and domain dependent.

As such eye-tracking in visualization and data space could be used to understand how users forage for, integrate, and hypothesize about data using complex, domain specific, visual analytics systems. This would be significantly different than understanding visual perception and low level visual strategies in static visualizations and can play an important role in advancing the visual analytics agenda [cook]. 

\textbf{Improving viewed object prediction and detection:} In section~\ref{sec:Methods} we describe two ways in which transitions probabilities between objects and viewing probabilities can be determined for an instrumented visualization: an intuitive way and a principled way. A third way can emerge from more research on this subject. Specifically, we hypothesize that there is a set of general principles about how people take in visual and data content that are valid across visualizations and can be used for the purpose of improving viewed object detection. Such principles include but are not limited to how people see highlighted objects, how people revisit object, how visual properties of an object (e.g., size, label, details) may influence how it is perceived, and what the interplay between manual interactions and visual interactions is.

Additional work is need to understand how to and whether we can detect visual objects other than nodes or labels, such as for instance polylines in a parallel coordinate plot, contours in a group or set visualization, or cells in a heatmap. It is unclear how a gaze score ($gs$) for such objects since there is no research to describe how people fixate such objects. Moreover, computing gaze scores for individual objects based on their shape alone and without taking into consideration the visual context that they are rendered in is a na√Øve approach. We have preliminary evidence that a curve drawn on an empty screen or is strongly highlighted, elicits a very different visual response than a curve that blends into its visual content. Such differences should be incorporated into how visual scores are computed and make dealing with arbitrary visualization content challenging. For instance, the connective curves drawn in our PivotPaths visualization have different weights; some curves are prominent and some are barely noticeable. Our instrumentation is currently limited in that it does not differentiate between such cases.

\textbf{Evaluating detection algorithms:} Evaluating the real impact of the intelligent gaze algorithm is hard as it depends on many factors. For example, if the visual content is sparse, then computing gaze scores alone would be sufficient and our algorithm would not create any benefit. The content of the visualization can also be more or less amenable to prediction. For example, we hypothesize that employing prediction in a parallel coordinates plot is much harder than doing it with well separated, point-like objects, such as nodes in a network. Even comparing our results to human coders is questionable in terms of evaluating our prediction algorithm since, if coders look primarily at momentary gaze positions, rather than trying to understand what users are trying to do overall, then their annotation may be closer to our raw gaze scores rather than the score that integrates prediction. 

Finally, we think human coders can provide a robust ground truth for evaluating techniques such as ours. We also hypothesize that such ground truths may be improved by combining them with a think aloud protocols. 