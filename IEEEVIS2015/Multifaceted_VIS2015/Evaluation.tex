\section{Evaluation}
\label{sec:Evaluation}
We collected data from $9$ subjects, each using our instrumented visualization for approximately $50$ minutes on a series of structured and unstructured tasks. We used this data data to test the validity and effectiveness of our approach in two ways. 

First, we compared the output of the intelligent algorithm to human annotation data. We found that data collected automatically was on average as similar to human annotations, as human annotations were similar to each other. Moreover, we conducted this analysis for all three viewed detection algorithms described in Sections~\ref{sec:AOIBasedViewedObjectDetection} to~\ref{sec:MehthodsIntelligentAlgorithm} and found that the AOI algorithm performs poorly compared to the other two, and that the predictive algorithm improves detection accuracy by about $5\%$  (Figure~\ref{fig:quantitative}). 

Second, we demonstrate that our instrumentation method can provide relevant information and can be leveraged in novel and interesting ways. Specifically, we show both qualitatively and quantitatively that viewed objects detected by our instrumentation are closely corelated to the tasks we asked people to do, and that data collected automatically from many users can answer novel questions about how people use visualizations.    

\subsection{Study Design }
\label{sec:EvalStudyDesign}

\noindent\textbf{Setup: } We used the visualization and data described in Section~\ref{sec:Methods}, and a lightweight $60$Hz EyeX eye-tracker from Tobii connected to a 17'' monitor. Subjects were seated approximately $30''$ away from the display. 

\noindent\textbf{Subjects:} We collected data from $9$ graduate and undergraduate students with ages ranging between $20$ years and $30$ years. Six of them were male and three were female. Subjects were paid $\$10$ for their participation. 

\noindent\textbf{Protocol:} Subjects were first given a description of the study's purpose and protocol. They were then introduced to our IMDB PivotPaths visualization and asked to perform a few training tasks to help them get accustomed with the visualization. This introductory part lasted on average $10$ minutes. The main section of the study followed, involved multiple instances of four types of tasks, and lasted approximately $50$ minutes. 

\noindent\textbf{Tasks:} We asked subjects to complete four types of tasks. We aimed to balance structured tasks and unstructured tasks. To solve the structured tasks, subjects had to consider a set of objects that was better defined and less variable than in unstructured tasks. This made it easier for us to test the degree to which our detection of viewed objects is aligned with the data required to complete the tasks. On the other hand, data collected for unstructured tasks may be better at informing designs of future analysis systems of such data. We limited the time we allowed subjects to spend on each task for two reasons: to manage the total duration of the study, and to make results comparable across time and users.

\begin{itemize}
\item \textbf{Task1 (structured):} Finding four commonalities between pairs of movies. The tasks were limited at three minutes each, and subjects solved the following four instances of this task: (a) Goodfellas and Raging bull; (b) Raiders of the Lost Ark and Indiana Jones and the Last Crusade; (c) Invictus and Million Dollar Baby; (d) Inception and The Dark Knight Rises.  
\item \textbf{Task2 (structured):} Ranking collaborations between a director and three actors ($2$ minutes).  Subjects completed four task instances centered around the following directors: (a) Ang Lee; (b) Tim Burton; (c) James Cameron; (d) David Fincher.  
\item \textbf{Task3 (semi-structured):} Given three movies, subjects were asked to recommend a fourth ($5$ minutes). Subjects solved three such tasks: (a) Catch Me If You Can, E.T. the Extra-Terrestrial, and Captain Phillips; (b) To Kill a Mockingbird, The Big Country, and Ben-Hur; (c) Inglourious Basterds, The Avengers, and Django Unchained.



\item \textbf{Task4 (unstructured):} Given a brief and incomplete description of the ``Brat Pack'', a group of young actors popular in the 80's, subjects were asked to find additional members and movies they acted in. Subjects solved one such task, in approximately $5$ minutes. 
\end{itemize}


\subsection{Results}
\subsubsection{Data collected automatically is similar to that of human annotators}
\label{sec:EvalResults}


We tested whether the outputs of the three algorithms described in Sections~\ref{sec:AOIBasedViewedObjectDetection} to~\ref{sec:MehthodsIntelligentAlgorithm} (AOI, probabilistic, and predictive) are comparable to annotation data obtained from human coders who inspected screen-captures with overlaid gaze samples and manually recorded what subjects looked at. As shown in Figure~\ref{fig:quantitative}, we found that the overlap between human annotations and the predictive algorithm' s output is similar to the overlap within the set of human annotations, and that the predictive algorithm outperforms the other two. 

Specifically, we enlisted the help of five coders and asked them to annotate eye-tracking data corresponding to one task of approximately three minutes, for each of six subjects.  The task was the same for all coders - task 1b. The six subjects were selected randomly and were the same for all five coders. Coders spent approximately one hour per subject completing their annotation. Four coders completed all six assigned annotation tasks, while one was able to annotate the data of only three of six subjects. 

Coders used an application that allowed them to browse through screen captures of a users' activity with overlaid gaze coordinates. We asked coders to advance through the videos in $100$ms time-steps, determine what visual objects their assigned subjects were viewing, and record those objects along with the start and end time of their viewing. If unsure which of multiple objects was viewed, coders were allowed to record all of them.  

We transformed each coder's annotation for each subject into temporal vectors with $100$ms resolution. These vectors contained at each position one or several objects that were likely viewed by the subject during the $100$ms time-step corresponding to that position. We then created similar representations from our automatically collected data. Finally, we defined a similarity measure between two such vectors as the percentage of temporally aligned cells from each vector that were equal. Equality between vector cells was defined as a non-empty intersection between their contents.  

For each algorithm, we computed the similarity of its output for each subject's data to all available annotations of the same data.  This yielded $4$ coders $\times$ $6$ subjects $+$  $1$ coders $\times$ $3$ subject $=$ $27$ similarities per algorithm. We averaged these similarities and plotted them as the first three bars in Figure~\ref{fig:quantitative}. Then, we compared each coder's annotation of a subject's data to all other available annotations of the same data. Since we had five annotations for three subjects, yielding $3$ subjects $\times$ $10$ annotation pairs $=$ $30$ similarities, and four annotations for the remaining three, yielding $3$ subjects $\times$ $6$ annotation pairs $=$ $18$ similarities, this process resulted in a total of $48$ similarities, which we averaged and plotted as the last bar of Figure~\ref{fig:quantitative}.

The data we collected allowed us to perform this analysis for all three algorithms described in Section~\ref{sec:MethodsAlgorithmsViewedObjectDetection}. Specifically, if we only consider gaze scores $gs$ that are equal to one (Section~\ref{sec:AOIBasedViewedObjectDetection}) and no predictive component, we essentially have the output of the AOI algorithm. If we limit the analysis to $gs$ scores alone, without the prediction component described in Section~\ref{sec:MehthodsIntelligentAlgorithm}, we have the output of the probabilistic approach described in Section~\ref{sec:ProbabilisticObjectDetection}.

\begin{figure}[htb]
  \centering
  \includegraphics[width=0.6\linewidth]{images/algosComparison.eps}
  \caption{Comparison between automated and manual viewed object detection.
The first three bars show the overlap between the outputs of
the three algorithms described in Section~\ref{sec:MethodsAlgorithmsViewedObjectDetection} and
annotation results of human coders. The last bar shows the overlap
within the set of human annotations. Values correspond to averages
over multiple tasks, multiple subject data sets, and multiple annotators,
and are computed as described in Section~\ref{sec:EvalResults}. Error bars
extend by one standard error. }
	\label{fig:quantitative}
\end{figure}

\subsubsection{Data collected automatically is relevant and useful}
\label{sec:EvalDataCollected}

We used two visual representations and analyses to show that data collected automatically is tightly correlated with the tasks that users had to do. We chose this evaluation for two reasons. First, it provides evidence that the automatic instrumentation approach can be used to solve the inverse problem: an observer or analyst who is unfamiliar with a subject's intentions can determine what these are by looking at the subject's visual interest in particular data. 

Second, it demonstrates how the automated collection of eye-tracking data can facilitate novel analyses and insights into how visualization are used. For example, our approach allowed us to quantify that a users' interest in a visual item present on the screen decays exponentially with a decrease in the items' relevance to a task. While it was generally known that users follow ``information scent'' when solving tasks visualy~\cite{informationscent2003},  we were now able to quantify this effect. 
\vspace{2mm}\noindent
\textbf{First }, we created heatmap representations from our collected data (Figure~\ref{fig:heatmap}) to illustrate qualitatively the strong connection between the tasks our subjects performed and the data we collected. We listed viewed objects vertically, discretized viewing scores by averaging them over $500ms$ intervals, and arranged them horizontally. Thus, time is shown horizontally, viewed objects vertically, and intensity of heatmap cells indicate the degree to which an object was viewed at a given moment in time. The viewed objects listed vertically were colored based on their type (movie, actor, director, genre) and could be sorted by either first time they were viewed, amount of viewing activity, or type.

Figure~\ref{fig:heatmap}, left, shows the data collected from a subject performing task 1b: finding commonalities between two Indiana Jones movies. The upper heatmap is ordered by the amount of visual attention that the subject dedicated to each element in the visualization. We notice that elements at the top of the heatmap are tightly connected to the subjects'  task.   In the bottom panel, viewed items are ordered by category (genre, director, movie, actor). We notice a clear temporal pattern: the movies involved directly in the task were viewed throughout the analysis, actors were considered early on, followed by genres, then directors, and ultimately a quick scan of other movies. We observed this pattern for most subjects and believe it was caused by the ordering used in the task's phrasing: we asked subjects to determine actors, genres, and directors that were common between the two movies.   

Figure~\ref{fig:heatmap}, right, shows a subject's results for one of the instances of task 3, which was significantly less structured than task 1 (see Section~\ref{sec:EvalStudyDesign}). This heatmap  was sorted by the first time each object was viewed and shows how subjects were moving through different aspects of the analysis. Heatmaps associated to these task types typically showed a wider range of viewed objects, as indicated by the heatmap's greater height. We attribute this pattern to the more exploratory nature of the task.  

\begin{figure*}[!ht]
  \centering
  \includegraphics[width=0.9\linewidth]{images/heatmaps.eps}
  \caption{Heatmap views of one subject's activity on two tasks; time, in 500ms increments, is shown horizontally; viewed objects are viewed vertically; cell darkness indicates viewing intensity (black: high; white: low). (Top left) Data for task 1b (see Section~\ref{sec:EvalStudyDesign}); viewed items are ordered by decreasing total amount they were viewed. (Bottom left) Data for task 1b; viewed items are ordered by category (genre, director, movie, actor). (Right) Data for task 3a; viewed items are ordered by first time they were viewed. 
}
	\label{fig:heatmap}
\end{figure*}

\vspace{2mm}\noindent
\textbf{Second }, we formalized the relevance of each visual item to a particular task and plot this relevance against the amount of interest that each item attracted, as shown in Figure~\ref{fig:RelevanceDiagram}. These plots quantify the degree to which tasks determine users' interest in visual items, and demonstrates that our instrumentation captures relevant data.    

We formalized the relevance of a visual item to a task as $\text{Relevance}~=~1/(1+d)$, where $d$  is the shortest graph distance between that item and items mentioned directly in the task description.  To exemplify, the relevance of Goodfellas and Ranging Bull to task 1a is $1$ as they are the focus of the task, that of Martin Scorsese is $1/2$  because he directed both movies, while that of other movies directed by Scorsese is $1/3$. This definition is not fully accurate as items might be relevant to a task even though they are not directly mentioned in the description.  For instance, items that eventually constitute a user's answer will elicit more attention. Moreover, this definition is particular to the visualization we instrumented.

Figure~\ref{fig:RelevanceDiagram} facilitates several insights. First, even though many items were shown to subjects during their tasks, only very few were viewed for significant periods of time, and many were not viewed at all. Second, the types of data that users focus on correlates with the particularities of each task. For example, task 3 involved movie recommendations and Figure~\ref{fig:RelevanceDiagram} illustrates that genres and directors were viewed significantly more than in task 4, which involved determining the identity of a group of actors and seemed to drive users to mostly focus their attention on actors. 

\begin{figure}[!htb]
  \centering
	\includegraphics[width=0.45\linewidth]{images/Legends.eps}
  \includegraphics[width=0.9\linewidth]{images/RelevanceDiagramTask1.eps}
	
	\includegraphics[width=0.61\linewidth]{images/RelevanceDiagramTask2.eps}
	
	\includegraphics[width=0.71\linewidth]{images/RelevanceDiagramTask3.eps}
	
	\includegraphics[width=\linewidth]{images/RelevanceDiagramTask4.eps}
	\includegraphics[width=0.45\linewidth]{images/Legends.eps}
	
  \captionof{figure}{Users' interest in data objects, in relation to each objects' relevance to a task, for twelve tasks of four types. Each individual task is plotted in its type's corresponding chart as a subdivision across multiple relevance categories. Relevance was computed as described in Section~\ref{sec:EvalDataCollected}, and plotted for all objects that were visible to subjects during each task. The average interest in objects with the same task relevance are linked by separate polylines for each individual task; errors bars extend from the averages by one standard error.}
	\label{fig:RelevanceDiagram}
\end{figure}

\begin{table}[htbp]	
	\centering
		\begin{tabular}{|c|c|c|c|c|c|}
			\hline
			 \multicolumn{2}{ |c| }{Movie to}  &\shortstack{No. of\\transitions} 	&\shortstack{ Observed \\trans.\\prob. }	&\shortstack{	 Unbiased\\trans.\\prob.} & \shortstack{Ratio \\$\frac{\text{Observed}}{\text{Unbiased}}$}\\ \hline
      \multirow{4}{*}{Actor}	&-	&793	&0.445	&0.898	&0.495	\\	\cline{2-6}
															&H	&147	&0.082	&0.02	&4.081	\\	\cline{2-6}
															&C	&228	&0.128	&0.052	&2.473	\\	\cline{2-6}
															&CH	&616	&0.345	&0.03	&11.484	\\	\hline
				\multirow{2}{*}{Movie}	&-	&5727	&0.761	&0.899	&0.846	\\	\cline{2-6}
																&H	&1798	&0.239	&0.101	&2.376	\\	\hline
				\multirow{4}{*}{Director}	&-	&304	&0.537	&0.887	&0.606	\\	\cline{2-6}
																	&H	&37	&0.065	&0.021	&3.088	\\	\cline{2-6}
																	&C	&51	&0.09	&0.055	&1.647	\\	\cline{2-6}
																	&CH	&174	&0.307	&0.038	&8.176	\\	\hline
				\multirow{4}{*}{Genre}	&-	&193	&0.33	&0.792	&0.417	\\	\cline{2-6}
																&H	&40	&0.068	&0.033	&2.045	\\	\cline{2-6}
																&C	&69	&0.118	&0.102	&1.159	\\	\cline{2-6}
																&CH	&282	&0.483	&0.072	&6.693	\\	
				\hline
		\end{tabular}		
		%\caption*{(a)Transitions from Movies}
		\smallskip
		
		\begin{tabular}{|c|c|c|c|c|c|}
			\hline
			 \multicolumn{2}{ |c| }{Actor to}    &\shortstack{No. of\\transitions} 	&\shortstack{ Observed \\trans.\\prob. }	&\shortstack{	 Unbiased\\trans.\\prob.} & \shortstack{Ratio \\$\frac{\text{Observed}}{\text{Unbiased}}$}\\ \hline
						 \multirow{2}{*}{Actor}	&-	&4711	&0.685	&0.962	&0.713	\\	\cline{2-6}
																		&H	&2164	&0.315	&0.038	&8.207	\\	\hline
							\multirow{4}{*}{Movie}	&-	&839	&0.469	&0.82	&0.572	\\	\cline{2-6}
																			&H	&213	&0.119	&0.058	&2.046	\\	\cline{2-6}
																			&C	&386	&0.216	&0.076	&2.843	\\	\cline{2-6}
																			&CH	&352	&0.197	&0.046	&4.284	\\	\hline
							\multirow{2}{*}{Director}	&-	&68	&0.701	&0.959	&0.731	\\	\cline{2-6}
																				&H	&29	&0.299	&0.041	&7.271	\\	\hline
							\multirow{2}{*}{Genre}	&-	&43	&0.524	&0.931	&0.563	\\	\cline{2-6}
																			&H	&39	&0.476	&0.069	&6.918	\\	
				\hline
		\end{tabular}
	%\caption*{(b)Transitions from Actors}
	\smallskip
	
	\begin{tabular}{|c|c|c|c|c|c|}
			\hline
			 \multicolumn{2}{ |c| }{Director to}    &\shortstack{No. of\\transitions} 	&\shortstack{ Observed \\trans.\\prob. }	&\shortstack{	 Unbiased\\trans.\\prob.} & \shortstack{Ratio \\$\frac{\text{Observed}}{\text{Unbiased}}$}\\ \hline
       \multirow{2}{*}{Actor}	&-	&71	&0.747	&0.958	&0.78	\\	\cline{2-6}
															&H	&24	&0.253	&0.042	&5.964	\\	\hline
			\multirow{4}{*}{Movie}	&-	&271	&0.494	&0.792	&0.623	\\	\cline{2-6}
															&H	&55	&0.1	&0.04	&2.478	\\	\cline{2-6}
															&C	&130	&0.237	&0.108	&2.198	\\	\cline{2-6}
															&CH	&93	&0.169	&0.06	&2.841	\\	\hline
			\multirow{2}{*}{Director}	&-	&384	&0.706	&0.93	&0.759	\\	\cline{2-6}
																&H	&160	&0.294	&0.07	&4.216	\\	\hline
			\multirow{2}{*}{Genre}	&-	&256	&0.522	&0.899	&0.581	\\	\cline{2-6}
															&H	&234	&0.478	&0.101	&4.708	\\	
				\hline
		\end{tabular}
		%\caption*{(c)Transitions from Director}
		\smallskip
		
		\begin{tabular}{|c|c|c|c|c|c|}
			\hline
			 \multicolumn{2}{ |c| }{Genre to}   &\shortstack{No. of\\transitions} 	&\shortstack{ Observed \\trans.\\prob. }	&\shortstack{	 Unbiased\\trans.\\prob.} & \shortstack{Ratio \\$\frac{\text{Observed}}{\text{Unbiased}}$}\\ \hline
			 \multirow{2}{*}{Actor}	&-	&61	&0.656	&0.9791	&0.67	\\	\cline{2-6}
															&H	&32	&0.344	&0.021	&16.47	\\	\hline
				\multirow{4}{*}{Movie}	&-	&229	&0.118	&0.261	&0.453	\\	\cline{2-6}
																&H	&46	&0.024	&0.008	&3.001	\\	\cline{2-6}
																&C	&172	&0.089	&0.093	&0.956	\\	\cline{2-6}
																&CH	&138	&0.071	&0.013	&5.288	\\	\hline
				\multirow{2}{*}{Director}	&-	&282	&0.591	&0.973	&0.608	\\	\cline{2-6}
																	&H	&195	&0.409	&0.027	&15.174	\\	\hline
				\multirow{2}{*}{Genre}	&-	&348	&0.398	&0.943	&0.422	\\	\cline{2-6}
																&H	&526	&0.602	&0.057	&10.627	\\	
				\hline
		\end{tabular}
		%\caption*{(d)Transitions from Genres}\smallskip
	\captionof{table}{Transitions from a source object to a target object, divided by: (i) type of source and target; (ii) whether the target was highlighted (H); (iii) whether the target was highlighted and connected to the source (HC); (iv) and whether source and target were neither highlighted nor connected. Columns show: (i) the number of direct transitions for the source/target combination; (ii) the observed transition probability from the source to that target; (iii) the (unbiased) probability of transition between source and target if all elements had equal probability to be viewed; (iv) the ratio between observed and unbiased transition probabilities.}
	\label{tab:TransitionFromMovie}
\end{table}



\subsubsection{Assumptions about viewing transition patterns hold}
\label{sec:EvalAssumptionAboutViewingTransition}
We performed a qunatitative analysis of our subjects' viewing-transition patterns, using the data we collected during our study, and found that the informal assumptions we made in (Section~\ref{sec:MehthodsIntelligentAlgorithm}) were correct: our users showed strong preferences to view objects that were hilighted or connected to previously viewed objects. The last three columns in Table~\ref{tab:TransitionFromMovie} compare the probability with which our users viewed one object category after another (e.g., viewed a highlighted actor afer a movie), as computed from data we collected, to a hypothesized base case in which users picking next items to view without any particular preferance. The quantitative results show for instance that after viewing a movie, our users were four times more likely to look at an actor that was highligted ($Ratio = 4.081$), and eleven times more likely to look at an actor that was both highlighted and connected to the previously viewed movie ($Ratio = 11.484$), than if users were viewing items at random.  


To reach these results, we first discarded the prediction component from the data we collected, since it represents exactly the assumption we seeked to evaluate. We then counted direct viewing transitions between all types of objects (sources) to all other types of objects (targets) and divided them into categories based on whether targets were highlighted, connected to the sources, or both (Table~\ref{tab:TransitionFromMovie}).  For example, after looking at a movie title, our users looked at an actor that was unconnected to that movie and unhighligted $793$ times and at an actor that was connect to the movie and highlighted $616$ times. Since in our viusalization connections existed only between movies and actors, genres, and directors, transitioning to a connected or highlighted and connected target was only possible when transitioning to and from movies.
    

These counts were translated into observed transition probabilities by normalizing them by the total number of transitions from each type of source to each type of category. For example , our users transitioned in total $1784$ times from a movie to an actor, of which $147$ transitions were from a movie to a highlighted actor, yielding an observed transition probability of $147 / 1784 = 0.082$.

However, interpreting these observed probabilities by themselves can be misleading. For example, we observed 793 transitions from a movie to an unconnected actor, and just 147 to a connected one. This however does not indicate a preferance for viewing actors that are not higlighted, but happened because users had many more opportunties to view unhilighted actors than they had to view highlighted ones. Intuitively, when a user transitions their gaze from a source to a target, the visualization typically contains many more targets that are not highlighted and are not connected to the source, than those that are. 

Instead, observed transitions should be compared to the default case which assumes that users treat all visual objects equally. Assume the following simplified case: a movie is connected to two of ten actors shown in a visualization. We observe that of ten transitions from that movie to one of the actors, five were to a connected actor, while five were to unconnected actors. The two observed probabilities, to connected and unconnected actors, would in this case be equal at $5/10 = 0.5$. However, if there was no transitioning preference, the probability of transitioning to any actor would be equal to $0.1$, that of transitioning to a connected actor $0.2$, while that of transitioning to an unconnected actor $0.8$. Thus, our observed transition probability from a movie to a connected actor is $0.5/0.2=2.5$ times higher than the default, unbiased probability, while our observed transition from a movie to an unconnected actor is a fraction ($0.5/0.8=0.625$) of the unbiased one.  

To compute unbiased probabilities, every time we counted a transition from a source to a target, we also counted all target options available to a user at that point, given the state and structure of the visualization at the time of transition. Reverting to our simplified example, for each of our ten observed transitions we would count two possible transitions to connected actors and eight possible transitions to unconnected actors, ending up with $20$ counts for connected actors, and $80$ counts for unconnected actors. These numbers allow us to compute the two unbiased probabilities as $20/(20+80)$ and $80/(20+80)$.


