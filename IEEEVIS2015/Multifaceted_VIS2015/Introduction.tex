\section{Introduction}

Eye-tracking allows us to locate where users are looking on a computer screen~\cite{ware1987evaluation,jacob1991use} and is often used to record peoples' gazes while they are performing tasks that involve visual stimuli, and to analyze this data off-line to see how people interpreted the stimuli and solved the tasks~\cite{duchowski2007eye}. Diagnostic eye-tracking has been used widely in psychology and cognitive science to help researchers understand thought and affect mechanisms~\cite{rayner1995eye}, and in data visualization and human computer interaction (HCI) to explain how people use visual interfaces~\cite{duchowski2007eye}. 

To date, eye-tracking data is collected and interpreted in a low-level form, as gaze-coordinates in the space of rendered visual stimuli that gazes were recorded for. Relating this data to the semantic content of the stimuli is generally done offline by human analysts or coders who inspect gaze heatmaps visually, or define area of interest (AOIs) manually. As such, this process requires significant manual intervention and is especially difficult for studies involving many subjects, long sessions, and interactive content.

The work we describe here rests on the observation that for visual content that a computer generates on the fly, such as data visualizations, the structure and layout of the visual content is known at rendering time. Thus, for data visualizations that are open to instrumentation, gaze-coordinates provided by an eye-tracker could presumably be related to the rendered content of the visualization in real-time, yielding a detailed account of visualization elements, and implicitly data elements, that users were viewing at any given time. Our paper shows that this instrumentation approach is indeed feasible and yields data that can be collected over long sessions involving open-ended tasks and interactive content. Moreover, the collected data is derived directly from the visualization's underlying data, and thus has semantic meaning without the need for additional coding. As such, this data can be particularly well suited to explain how people forage for, analyze, and integrate information in complex visual analytics systems and workflows. 
While this idea is similar to previous work on 3D objects of interest (OOI)~\cite{stellmach20103d}, and  dynamic AOIs~\cite{papenmeier2010dynaoi}, detecting individual data and visualization objects that are being viewed (e.g., nodes in a network) is different than binning gazes into traditional AOIs, which are typically large, non-overlapping, and lack data-derived semantic meaning. In this paper we evaluate the feasibility of collecting data that is highly granular and has semantic meaning. This creates interesting opportunities for data analysis, which we discuss in Section~\ref{sec:Discussion}, but also involves a challenge: how can we accurately use eye-tracking data, which is generally imprecise and low resolution, to discriminate between many small, intertwining visual objects which are typical of data visualization? First, we show that a fuzzy interpretation of gaze data, that is detecting likelihood rather than certainty that an object was viewed, can work well for practical purposes. Second, we build on previous work by Salvucci, who showed that viewed object detection can be significantly more accurate if gazes are ``interpreted intelligently'' by leveraging the structure, semantics, and tasks of an instrumented interface to predict which objects are most likely to be viewed at a given time~\cite{salvucci1999inferring, salvucci2000intelligent}. Specifically, we introduce and evaluate an algorithm that can ``intelligently'' detect viewed objects from gaze coordinates in real-time, and that is tailored to visualization content. 

To evaluate the feasibility of collecting and analyzing eye-tracking data directly in visualization and data space,  we used the aforementioned algorithm to instrument an interactive visualization of IMDB data and collect viewed-object data from nine subjects. We then showed that the instrumentation yielded useful results in two ways. First, we show that viewed objects identified by our instrumentation method are tightly related with tasks we asked users to do. Second, we show that differences between the manual annotations of four coders who were asked to analyze the raw gaze data, and our automatically collected data, are not greater than differences between the coders' own annotations. As part of this quantitative evaluation, we also show that our ``intelligent'' algorithm for detecting viewed objects outperforms two naïve implementations. 

Our \textbf{contributions} are: (i) a qualitative and quantitative demonstration of the validity, effectiveness, and potential of analyzing eye-tracking data in visualization and data space; (ii) an ``intelligent'' algorithm for detecting viewed objects from eye-tracking data in visualizations that are open for instrumentation, and its quantitative evaluation; (iii) a discussion of methods and benefits for collecting and analyzing eye-tracking data in visualization and data space.
