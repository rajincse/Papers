\section{Introduction}

Eye-tracking allows us to locate where users are looking on a computer screen~\cite{ware1987evaluation,jacob1991use} and is often used to record peoples' gazes while they are performing tasks that involve visual stimuli, and to analyze this data off-line to see how people interpreted the stimuli and solved the tasks~\cite{duchowski2007eye}. Diagnostic eye-tracking has been used widely in psychology and cognitive science to help researchers understand thought and affect mechanisms~\cite{rayner1995eye}, and in data visualization and human computer interaction (HCI) to explain how people use visual interfaces~\cite{duchowski2007eye}. 
To date, eye-tracking data is collected and interpreted in a low-level form, as gaze-coordinates in the space of rendered visual stimuli that gazes were recorded for. Relating this data to the semantic content of the stimuli is generally done offline by human analysts or coders who inspect gaze heatmaps visually, or define area of interest (AOIs) manually. As such, this process requires significant manual intervention and is especially difficult for studies involving many subjects, long sessions, and interactive content.

The work we describe here rests on the observation that for computer generated visual content, the layout of the visualization is known at rendering time. Thus, for data visualizations that are open to instrumentation, gaze coordinates provided by an eye-tracker can be related to the rendered content of the visualization in real-time, yielding a detailed account of visualization elements, and implicitly data elements, that users were viewing at any given time. We show that such instrumentation yields data that can be collected over long sessions involving open-ended tasks and interactive content, and, being derived from the visualization's data, has semantic meaning without the need for additional coding. As such, this data can be particularly well suited to explain how people forage for, analyze, and integrate information in complex visual analytics systems. 
While this idea is similar to previous work on objects of interest (OOI)~\cite{papenmeier2010dynaoi}, and  dynamic AOIs~\cite{papenmeier2010dynaoi}, detecting individual data and visualization objects that are being viewed (e.g., nodes in a network) is different than binning gazes into traditional AOIs, which are typically large, non-overlapping, and lack data-derived semantic meaning. We collect data that is highly granular and has semantic meaning. This creates interesting opportunities for data analysis, which we discuss in section~\ref{sec:Methods}, but also involves a challenge: how can we accurately use eye-tracking data, which is generally imprecise and low resolution, to discriminate between many small, intertwining visual objects which are typical of data visualization? First, we show that a fuzzy interpretation of gaze data, that is detecting likelihood rather than certainty that an object was viewed, can work well for practical purposes. Second, we build on previous work by Salvucci, who showed that viewed object detection can be significantly more accurate if gazes are ``interpreted intelligently'' by leveraging the structure, semantics, and tasks of an instrumented interface to predict which objects are most likely to be viewed at a given time~\cite{salvucci1999inferring, salvucci2000intelligent}. Specifically, we introduce and evaluate an algorithm for “intelligent” gaze interpretation that is tailored to visualization content. 

We evaluated the effectiveness of our approach by instrumenting an interactive visualization of IMDB data, collecting data from ten subjects using it, and leveraging the recorded  data to demonstrate that collecting and analyzing eye-tracking data is feasible. First, we show that viewed objects identified by our instrumentation method are tightly related with tasks we asked user to do. Second, we show that differences between the manual annotations of four coders and our automatically collected data, are not greater than differences between the coder's own annotations. As part of this quantitative evaluation, we also show that our ``intelligent'' algorithm for detecting viewed objects outperforms a naïve implementation. Third, we show a few examples of how the particularities of the data we collect can be leveraged in its analysis. 

Our \textbf{contributions} are: (i) a qualitative and quantitative demonstration of the validity, effectiveness, and potential of analyzing eye-tracking data in visualization and data space; (ii) an ``intelligent'' algorithm for detecting viewed objects from eye-tracking data in visualizations that are open for instrumentation, and its quantitative evaluation; (iii) a discussion of collecting and analyzing eye-tracking data in visualization and data space.
