\section{Introduction}

\IEEEPARstart{E}{ye}-tracking allows us to locate where users are looking on a computer screen~\cite{ware1987evaluation,jacob1991use} and is often used to record peoples' gazes while they are performing tasks that involve visual stimuli, and to analyze this data off-line to see how people interpreted the stimuli and solved the tasks~\cite{duchowski2007eye}. Diagnostic eye-tracking has been used widely in psychology and cognitive science to help researchers understand thought and affect mechanisms~\cite{rayner1995eye}, and in data visualization and human computer interaction (HCI) to explain how people use visual interfaces~\cite{duchowski2007eye}. 

To date, eye-tracking data is collected and interpreted in a low-level form, as gaze-coordinates in the space of rendered visual stimuli that gazes were recorded for. Relating this data to the semantic content of the stimuli is generally done offline by human analysts or coders who inspect gaze heatmaps visually, or define area of interest (AOIs) manually. As such, this process requires significant manual intervention and is especially difficult for studies involving many subjects, long sessions, and interactive content.

The work we describe here rests on the observation that for visual content that a computer generates on the fly, such as data visualizations, the structure and layout of the visual content is known at rendering time. Thus, for data visualizations with source code that is open to instrumentation, gaze-coordinates provided by an eye-tracker can be related to the rendered content of the visualization in real-time, yielding a detailed account of visualization elements, and implicitly data elements, that users are viewing. Our paper shows that this instrumentation approach is indeed feasible and produces data that can be collected over long sessions involving open-ended tasks and interactive content. Moreover, the collected data is derived directly from the visualization's underlying data, and thus has semantic meaning without the need for additional coding. As such, this data can be particularly well suited to explain how people forage for, analyze, and integrate information in complex visual analytics systems and workflows. 

While this idea is similar to previous work on 3D objects of interest (OOI)~\cite{stellmach20103d}, and  dynamic AOIs~\cite{papenmeier2010dynaoi}, detecting individual data and visualization objects that are being viewed (e.g., nodes in a network) is different than binning gazes into AOIs, which are traditionally large, non-overlapping, and lack data-derived semantic meaning. Instead, we evaluate the feasibility of collecting data that is highly granular and has semantic meaning. This creates interesting opportunities for data analysis, which we exemplify in Section~\ref{sec:Evaluation} and discuss in Section~\ref{sec:Discussion}, but also involves a challenge: how can we accurately use eye-tracking data, which is generally imprecise and low resolution, to discriminate between many small, intertwining visual objects which are typical of data visualization content?  

First, we show that a fuzzy interpretation of gaze data, that is detecting likelihood rather than certainty that an object was viewed, can work well for practical purposes. Second, we build on previous work by Salvucci, who showed that viewed object detection can be significantly more accurate if gazes are ``interpreted intelligently'' by leveraging the fact that users don't view visual objects in random order, but in sequences and patterns that are influenced by tasks and visualization properties. For example, as we show in Section~\ref{sec:Evaluation}, users tend to look at highlighted items rather than regular ones, and they search for information connected to what they viewed previously. Such information can be used to predict which objects are most likely to be viewed at a given time~\cite{salvucci1999inferring, salvucci2000intelligent}. We contribute by introducing and evaluating an algorithm that can ``intelligently'' detect viewed objects from gaze coordinates in real-time and that is tailored to visualization content, and we reveal and quantify viewing patterns such as those described above for one specific visualization.  

To evaluate the feasibility of collecting and analyzing eye-tracking data directly in visualization and data space,  we used the aforementioned algorithm to instrument an interactive visualization of IMDB data and collect viewed-object data from nine subjects. We then showed that the instrumentation yielded useful results in two ways. First, we show that viewed objects identified by our instrumentation method are tightly related with tasks we asked users to do. Second, we show that differences between the manual annotations of five coders who were asked to analyze the raw gaze data, and our automatically collected data, are not greater than differences between the coders' own annotations. As part of this quantitative evaluation, we also show that our novel algorithm for detecting viewed objects outperforms two na\"{\i}ve implementations. 

Our \textbf{contributions} are: (i) a qualitative and quantitative demonstration of the validity, effectiveness, and potential of analyzing eye-tracking data in visualization and data space; (ii) an ``intelligent'' algorithm for detecting viewed objects from eye-tracking data in visualizations that are open for instrumentation, and its quantitative evaluation; (iii) a demonstration of the existence of viewing patterns in visualization and a methodology to compute them; (iv) a discussion of methods, benefits, and applications for collecting and analyzing eye-tracking data in visualization and data space.

