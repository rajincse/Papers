\section{Methods}\label{sec:Methods}

Our general approach is illustrated in Figure~\ref{fig:systemBlockDiagram}. For visualizations with code that is open to instrumentation, gaze coordinates provided by eye-trackers can be mapped to visual objects displayed on the screen automatically and in real-time, since the computer generated visual content and its layout is known during rendering.  Specifically, a visualization instrumented with our approach will not only draw visual primitives on the screen (e.g., nodes in a graph), but will also inform a viewed-object detection algorithm about where such primitives are drawn and their shape. To this end the instrumentation requires that object-rendering commands are mirrored with calls to an instrumentation library within the visualization's source code. The viewed-object detection algorithm uses this information online to map 2D gaze coordinates to visualization objects rendered on the screen. Should the visualization be transformed (e.g., zoomed, panned), its content altered (e.g., visual objects added or removed), or individual visualization components moved (e.g., dragging a node), the detection module is informed about these changes as soon as the visualization is redrawn and will map subsequent gaze samples to the new visual layout. 

\begin{figure}[htb]
  \centering
  \includegraphics[width=0.9\linewidth]{images/systemBlockDiagram.eps}
  \caption{Real-time detection of viewed objects in generative visualizations.}
	\label{fig:systemBlockDiagram}
\end{figure}

Next, we describe an effective algorithm for mapping gaze samples to objects that users are viewing. As shown in Figure~\ref{fig:systemBlockDiagram}, we will assume that gaze samples have already been transformed from screen space into model space. As such, the basic input of our algorithm is a stream of gaze samples in model space, and a list of visual primitives drawn on the screen, together with their shape and position. Naturally, as most modern visualizations are interactive, this visual structure of the visualization is likely to be highly dynamic. The algorithm outputs a stream of viewed visualization and data primitives (e.g., nodes, labels) in real-time, as users are viewing them in the instrumented visualization. Of course, our approach is limited in that it cannot be applied to already rendered images or videos and requires that the visualization's source code can be altered. We discuss these limitations in Section~\ref{sec:Limitations}.

We will describe our algorithm incrementally in the following three sections, starting from a na\"{\i}ve approach that simply draws AOIs dynamically around visualization objects, to a predictive one that detects objects more accurately by using knowledge about how specific visualizations are typically used.  A comparative evaluation of these three object detection algorithms is presented in Section~\ref{sec:EvalResults}.

\subsection{Algorithms for viewed object detection in data visualizations}
\label{sec:MethodsAlgorithmsViewedObjectDetection}
\subsubsection{AOI-based viewed object detection}
\label{sec:AOIBasedViewedObjectDetection}
A na\"{\i}ve viewed object detection approach is to treat object shapes as dynamic AOIs and determine that a viewed object is that with the most recent fixation landing in its AOI. This is a natural first try at detecting viewed visual objects from gaze data automatically, given that manually drawn AOIs are typically used in the same manner in offline eye-tracking data analysis, and that the similar concept of objects of interest (OOIs) has been proposed already by Stellmach et al.~\cite{stellmach20103d} for generative 3D content.

The problem with this approach is that for highly granular visual content, such as individual nodes or labels, users often fixate in the vicinity of the object rather than on the object itself. We demonstrate and quantify this observation in Section~\ref{sec:Evaluation}. A potential solution to this problem could be to make object AOIs slightly larger than the objects themselves. However, larger AOIs may lead to AOI overlap in cluttered visualizations and to the inability to assign a gaze sample or fixation to any single AOI. Ultimately, the problem lies with an inability to determine with absolute certainty what a user is looking at, and is described in more detail in the next section.

\subsubsection{A probabilistic approach to viewed object detection}
\label{sec:ProbabilisticObjectDetection}
Unlike mouse input, eye-tracking can only indicate a small screen region that a user is fixating, rather than a particular pixel. Typically, such a region is about one inch in diameter, though specific values depend on the user's distance to the monitor, and is determined by how human vision works. As such, we argue that it is generally impossible to tell with absolute certainty which object a user is viewing, if the user is fixating in the vicinity of multiple close objects (Figure~\ref{fig:discreminateFig4}(a)). This is not a significant problem for traditional AOI analyses, which generally use large AOIs. However, our goal is to detect the viewing of granular visual content, such as network nodes or glyphs, in cluttered visualizations. 

\begin{figure}[htb]
  \centering
  \includegraphics[width=\linewidth]{images/discreminateFig4.eps}
  \caption{(a) A real visualization example in which a user fixates in the vicinity of multiple close object groups (red dot). (b) predictive method: even though the latest gaze sample falls equidistantly between visual objects $O_3$ and $O_4$, we suspect that $O_3$ is the more likely viewing target given that (i) it is highlighted, and (ii) it is connected to $O_1$, which is likely to have been the object that the user viewed previously ($vs_1=0.6 > vs_2 = 0.4$). }
	\label{fig:discreminateFig4}
\end{figure}

As such, we advocate for a fuzzy interpretation of gaze data and detecting likelihoods that objects are viewed rather than certainties. To this end, we can compute object gaze scores $gs$ (for all objects $i$ in a visualization, and at all times $t$) that range between zero- the object is not viewed, and one- the object is certainly viewed as shown in Figure~\ref{fig:gazeScoreFig3} and Formula~\ref{eq:GS}. 

\begin{figure}[htb]
  \centering
  \includegraphics[width=0.5\linewidth]{images/gazeScoreFig3.eps}
  \caption{Calculating a gaze score $gs$ for a given object and a gaze sample landing nearby, where $d$ is the distance from the object to the gaze sample, and $R$ approximates the size of the user's foveated region.}
	\label{fig:gazeScoreFig3}
\end{figure}

%Formula 1
\begin{equation}
gs_{i,t} = 1 - \min (1, (\frac{d}{R}))
\label{eq:GS}
\end{equation}

The region of radius $R$ used in the formula is analogue to the user's foveated region, and as such needs to be constant in screen space. Thus, if the view is zoomed in or out, $R$ needs to be scaled accordingly in model space to remain constant in screen space.  A more detailed discussion about choosing an appropriate $R$ is given in Section~\ref{sec:Discussion}. Similar approaches were used by Salvucci et al.~\cite{salvucci2000intelligent} and Okoe et al.~\cite{okoe2014gaze}.

Finally, we note that the object scores ($gs$) do not directly equate to probabilities. The distinction is important because it implies that our implementation can detect two objects as being viewed simultaneously ($gs_1 = 1$ and $gs_2=1$). We think this is appropriate since a person can in fact visually parse multiple objects at the same time if they fall within the user's foveated region, and even think of multiple objects as a unit for specific task purposes. We discuss this in more detail in Section~\ref{sec:DiscussionEvaluatingViewedObjectDetection}. 

\subsubsection{A predictive algorithm for viewed object detection in data visualization}
\label{sec:MehthodsIntelligentAlgorithm}
Salvucci and Anderson described the concept of ``intelligent gaze interpretation'' in the context of a gaze-activated interface ~\cite{salvucci2000intelligent}. Their method achieved better accuracy in detecting which interface control a user is gazing at, by integrating both the proximity of the gaze to the control, and the likelihood that the control would be the target of a gaze-interaction, based on the current state and context of use of the interface. Formally, their algorithm identifies the most likely currently viewed item $i_{viewed}$ by solving 

\begin{equation*}
i_{viewed} = \displaystyle \argmax_{i\in I}[Pr(g|i)\cdot Pr(i)]
\end{equation*}

where $Pr(g|i)$ is the probability of producing a gaze at location $g$ given the intention of viewing item $i$, and $Pr(i)$ is the prior probability of an item $i$  being the target of a gaze-interaction. In Salvucci and Anderson's proof of concept implementation these prior probabilities were based on assumptions about how an interface might be used and were hardcoded into the system.  


We adapt Salvucci and Anderson's paradigm to more accurately determine which object a user is viewing, in the ambiguous case when a gaze-sample lands close to multiple objects (e.g.,  Figure~\ref{fig:discreminateFig4}(a).  

For example, in a network visualization we may assume that a user who has just viewed a node $n$ will more likely view one of $n$'s neighbors than a random other node, perhaps especially if the user previously highlighted node $n$ and its outgoing edges.  In Section~\ref{sec:Evaluation} we show qunatitatively that this assumption is in fact true for one visualization we tested.  

We consider a simplified such scenario in Figure~\ref{fig:discreminateFig4}(b): four visual objects ($O_{1\ldots 4}$), two of which are connected ($O_1$ and $O_3$), and one of which is highlighted ($O_3$), are shown on the screen. A new gaze sample registers between $O_3$ and $O_4$ at time $t$. Intuitively, it is more likely that $O_3$ was viewed since it is highlighted. Moreover, if we knew that $O_1$ was viewed just before the current moment, and, as described above we assume that users generally view neighboring nodes together, then this likelihood becomes even stronger.         
 
Formally, we compute $vs_{i,t}$  (i.e., the viewing score $vs$ of object $i$ at time $t$) by weighing the gaze score $gs_{i,t}$ described in Section~\ref{sec:ProbabilisticObjectDetection} by a prediction score $ps_{i,t}$ that object $i$ is a viewing target at time $t$:  
%Formula 2
\begin{equation}
vs_{i,t} = gs_{i,t} \times ps_{i,t}
\label{eq:VS}
\end{equation}

This prediction score is computed based on the likelihood that the object is viewed given the current state of the visualization (e.g., the object is highlighted), and the likelihood that it is viewed if some other specific object (e.g.,  a node's neighbor) was viewed just before it. Those two components are formalized by the  $\alpha$ score and $\beta$ score in Formula~\ref{eq:ps}, and are described below. 

%Formula 3
\begin{equation}
ps_{i,t} = \alpha_{i,t} \times \beta_{i,t}
\label{eq:ps}
\end{equation}

First, we will assume $\alpha$ is given as an input to our algorithm. Concrete examples of what $\alpha$ could be linked to are whether an object is highlighted (larger alpha) or not, whether an object is part of a group of objects recently queried by the user, or whether an object is known to be of particular interest to the users' current workflow (e.g., because they have viewed it often before, because the visualization was constructed using those objects as initial seeds, because they are mentioned as keywords in a user's profile). 

Second, we will compute $\beta$ based on a viewing transition function $T$ between objects:  $T(j,i)$ gives the likelihood that object $i$ is viewed after object $j$ is viewed. We will again assume that $T(j,i)$ is given as input to our algorithm. Concrete examples of what $T(j,i)$ could be linked to are whether objects $i$ and $j$ are somehow connected or related. This connection could be either visual, such as an explicit edge or leader line or an implicit sharing of similar visual attributes (e.g., color, shape), or semantic (e.g., both nodes are actors). 

To compute $\beta$, we could consider $\beta_{i,t} = T(j,i)$ but that would involve knowing $j$, the previously viewed object, with absolute certainty. This is problematic because we often cannot unequivocally determine which item was viewed at any given time. For example, as illustrated in Figure~\ref{fig:discreminateFig4}(b), $O_1$'s previous viewing score ($vs_{1,t-1}=0.6$), is just slightly larger than $O_2$'s viewing score ($vs_{2,t-1}=0.4$), and thus an absolute choice of $O_1$ over $O_2$ as previously viewed element would be rather arbitrary.  In other words, we cannot say with absolute certainty which of the two objects was viewed before because the user fixated between them. 

In more general terms, our computation of $\beta_{i,t}$ must account for multiple items $j$ that may have been viewed before. These items $j$ are those with a previous visual score $vs_{j,t-1}$ that is greater than $0$.  As such, we compute $\beta_{i,t}$ as a weighted average of all transition probabilities from objects $j$ with $vs_{j,t-1} > 0$ , to our current item $i$. The weights are given exactly by the likelihood that an object $j$ was viewed before - in other words by its previous viewing score $vs_{j,t-1}$. This computation is captured by Formula~\ref{eq:beta}.  

%Formula 4
\begin{equation}
\beta_{i,t} = \frac{\displaystyle\sum_{j} {vs_{j,t-1} \times T(j,i)}}{\displaystyle\sum_{j} vs_{j,t-1}} \text{ , where  } \parbox{15em}{$0\leq i \leq n$ and $gs_{i,t} > 0$\\$0\leq j \leq n$ and $vs_{j,t-1} > 0 $\\ $0\leq k \leq n$ and $gs_{k,t} = 0$ }
\label{eq:beta}
\end{equation} 

Finally, an important constrained needed to be added to Formula~\ref{eq:beta}. Intuitively, our approach means that previously viewed objects $j$ act as referees with varying degrees of influence (i.e., previous visual scores) in a competition between currently viewed items $i$. This analogy provides the intuition for the necessary constraint: an object should not referee a competition that it is part of. For example, in our simplified scenario, using $O_3$ as a previous element in a competition between itself and $O_4$ would result in an open feedback-loop and should be avoided. This restriction is reflected in Formula~\ref{eq:beta} by the 3rd inequality.  The algorithm pseudocode is provided in Algorithm~\ref{alg:ObjectDetection}.

\begin{algorithm}
\caption{Viewed Object Detection Algorithm}
\label{alg:ObjectDetection}
\begin{algorithmic}[1]
\State \textbf{Inputs: } 
\Statex $O_{i, \ldots, n}$= tracked visualization objects (shapes, positions)
\Statex $g(x,y) = $ gaze sample in model space (time $t$)
\Statex $\alpha_{i, \ldots, n} = $ view weights ($\alpha_{i, \ldots, n} \in [0,1]$)
\Statex $T(i,j) = $ viewing transition function ($T(i,j) \in [0,1]$)
\State \textbf{Outputs:}
\Statex $vs_{i,t} = $ momentary viewing scores of all objects ($i = 1, \ldots, n$). 
\For{$i \gets 1 \text{ to } n$}
	\State Compute $gs_{i,t}$	using Formula~\ref{eq:GS}
\EndFor
\State $max \gets 0$
\For{$i \gets 1 \text{ to } n$}
	\If{$gs_{i,t} > 0$} \label{algLine:PredictionScoreLines}
		\State Compute $\beta_{i,t}$	using Formula~\ref{eq:beta}
		\State $ps'_{i,t} \gets \alpha_{i,t} \times \beta_{i,t}$
		\If{$ps'_{i,t} > max$}
			\State $max \gets ps'_{i,t}$
		\EndIf
	\EndIf
\EndFor
\For{$i \gets 1 \text{ to } n$}
	\State $vs_{i,t} \gets gs_{i,t} \times \frac{ps'_{i,t}}{max} $
\EndFor
\end{algorithmic}
\end{algorithm}

Lastly, we note two more factors. First, to optimize for speed, we only compute prediction scores for objects with non-zero gazes (Algorithm~\ref{alg:ObjectDetection}, line~\ref{algLine:PredictionScoreLines}). Second, in our implementation we compute viewing scores for every gaze sample, rather than every fixation. We believe that doing so leads to results that are less dependent on how fixations are computed and more robust. Since our eye-tracker's sampling rate is $60$Hz, the scores $vs_{j, t-1}$ were computed just $15$ms ago, an interval generally shorter than the time it takes for people to shift their attention to a new object. As such, instead of using the raw $vs_{j,t-1}$ score, we use an average of the last several viewing scores, and, for all practical purposes, the term $vs_{j,t-1}$ should be replaced in the previous formulas by $ \sum_{k=1}^{k=15}{vs_{j,t-1}}$, which, given our eye-tracker's 60Hz temporal resolution, averages samples over approximately 250ms, a time window we observed to be close to an average fixation duration.  

However, we note that our algorithm can take as input fixations rather than individual gaze samples, in which case this step would not be necessary. Moreover, additional smoothing and filtering such as those summarized by Kumar et al.~\cite{kumar2008improving} could be used as a pre-processing step to clean gazes before feeding them into our algorithm. For example, we tried removing gaze samples with high velocity as they are likely to be part of saccades, but observed no discernable improvement in our algorithm's output. 


{\bf Performance analysis:} The algorithm needs to swift through all tracked elements ($n$) to find those in the proximity of a gaze sample or fixation ($k_t$). Then, to compute the term $\beta$ for each of the $k_t$ potentially viewed elements, the algorithm will iterate over $k_{t-1}$ objects with non-zero viewing scores from the previous iteration. As such, the algorithm is linear if we consider the number of objects that a user can view at any time to be a constant. However, that is not necessarily true since in special cases the entire visualization may fall within the algorithm's $R$ radius (e.g., the visualization is zoomed out too much). However, in this case the output of the algorithm would be irrelevant anyway and the algorithm should not be run. Thus, the algorithm is limited primarily by the clutter of the visualization, rather than the amount of computation. 


\subsection{Instrumenting a concrete visualization}
\label{sec:InstrumentingVisualization}

\begin{figure*}[htb]
  \centering
  \includegraphics[width=\linewidth]{images/pivotpaths.eps}
  \caption{PivotPaths visualization of IMDB data. Movies are displayed in the center of the screen, actors at the top, and directors and genres share the bottom space. Actors, directors, and genres associated to movies are connected through curves. Users can highlight objects and their connected neighbors by hovering over them.}
	\label{fig:pivotpaths}
\end{figure*}
We have used the previously described principles to instrument Doerk's interactive PivotPaths visualization of multifaceted data~\cite{dork2012pivotpaths}, which we linked to the popular internet movie database (IMDB). Shown in Figure~\ref{fig:pivotpaths}, the visualization renders movies in the center of the screen, actors on top, and genres and directors at the bottom. Actors, directors, and genres are connected by curves to the movies they associate with, and are larger, and their connections more salient, if they are associated with multiple movies. Actors, genres, and directors are colored distinctively, which is particular important for genres and directors since they occupy the same visual space. Such views are created in response to users' searches for specific movies, actors, and directors, and show data that is most relevant to the search. As shown in Figure~\ref{fig:pivotpaths}, users can hover over visual elements to highlight them and their connections. Users can also click on visual elements to transition the view to one centered on the select element. Finally, users can freely zoom and pan. 

We opted to instrument this particular visualization for three reasons. First, it is highly interactive and would thus be significantly difficult to analyze using manual gaze data analysis. Second, it contains visual metaphors, graphic primitives, and interactions typical of a wide range of visualizations. Third, we used the popular IMDB data source to leverage the familiarity of our prospective user-study subjects with it.  

To apply the previously described instrumentation algorithm to this visualization, we had to choose appropriate values for the $\alpha$ and $\beta$ factors used in Formula~\ref{eq:ps}. To this end, we made informal assumptions about how the visualization may be used, a method also employed by Salvucci~\cite{salvucci2000intelligent}. For instance, we assumed that transitions between connected items would occur more often than between unconnected items. We also assumed that elements that are hovered or highlighted are more likely to be viewed than those that are not. We translated these assumptions into specific weights, as exemplified in Table~\ref{tab:Transition2}. As we will show in Section~\ref{sec:Evaluation}, these assumptions hold for the pivot paths visualization and the nine subjects that used it in our study.

A more principled way to determine typical viewing patterns and sequences in a specific visualization is to run a pilot study and collect data using the algorithm described in Section~\ref{sec:ProbabilisticObjectDetection}, which does not require the $\alpha$ and $\beta$ inputs. Such preliminary data could be used to determine typical usage patterns and help refine viewed object detection by informing the choice of appropriate $\alpha$ and $\beta$ factors. We show how such an analysis can be done in Section~\ref{sec:Evaluation}. 

\begin{table}[htbp]
	\centering
		\begin{tabular}{|l|c|}
			\hline
			 \multicolumn{2}{|c|}{Assumed visual and transition weights} \\ \hline
			Movie to unconnected actor & 0.3\\\hline
			Movie to connected actor & 1\\\hline
			Movie to unconnected genre & 0.2\\\hline
			Movie to connected genre & 0.8\\\hline
			Movie to unconnected director & 0.3\\\hline
			Movie to connected director & 1\\\hline
			Any object hovered & 1\\\hline
			Any object not hovered & 0.5\\
			\hline			
		\end{tabular}
	\caption{Transition probabilities in our instrumented visualization (assumed).}
	\label{tab:Transition2}
\end{table}

Finally, as part of instrumentation, our system collected application screen shots, interactive events (e.g., hovering, zooming, panning), raw gaze samples captured at a rate of $60$Hz, and visual elements with non-zero viewing scores computed at the same rate of $60$Hz. For each viewed element we recorded the type (i.e., movie, actor, director, genre), its label, its gaze score ($gs$), its prediction score ($ps$), and the aggregated viewing score ($vs$). All recorded data was time stamped.
