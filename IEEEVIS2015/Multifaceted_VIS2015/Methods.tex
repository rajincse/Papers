\section{Methods}\label{sec:Methods}
%This section described a novel instrumentation method that leverages eye-tracking to detect in real-time which rendered objects users are looking at while working with a data visualization.  We present a general approach, details about the instrumentation of a specific data visualization, and a visual analysis system which we built to analyze eye-tracking data we collected in this way. 
%
%\subsection{Detecting viewed objects in data visualization content}
%
%\subsubsection{General Approach}
Our general approach is illustrated in Figure~\ref{fig:systemBlockDiagram}. For visualizations that are open to instrumentation, gaze coordinates provided by eye-trackers can be mapped to visual objects displayed on the screen automatically and in real-time, since the computer generated visual content and its layout is known during rendering.  Specifically, a visualization instrumented with our approach will not only draw visual primitives on the screen (e.g., nodes in a graph), but will also inform a viewed-object detection algorithm about where such primitives are drawn and their shape. The viewed-object detection algorithm uses this information to map 2D gaze coordinates to visualization objects rendered on the screen. Should the visualization be transformed (e.g., zoomed, panned), its content altered (e.g., filtering), or individual visualization components moved (e.g., dragging a node), the detection module is informed about these changes as soon as the visualization is redrawn and will map subsequent gaze samples to the new visual layout. 

\begin{figure}[htb]
  \centering
  \includegraphics[width=\linewidth]{images/systemBlockDiagram.eps}
  \caption{Real-time detection of viewed objects in generative visualizations.}
	\label{fig:systemBlockDiagram}
\end{figure}

Next, we describe an effective algorithm for mapping gaze samples to objects users are viewing. As shown in Figure~\ref{fig:systemBlockDiagram}, we will assume that gaze samples have already been transformed from screen space into model space. As such, the basic input of our algorithm is a stream of gaze samples in model space, and a list of visual primitives drawn on the screen, together with their shape and position. The algorithm outputs a stream of viewed visualization and data primitives (e.g., nodes, labels) in real-time, as users are viewing them in the instrumented visualization. Of course, our approach is limited in that it cannot be applied to already rendered images or videos. 

We will describe our algorithm incrementally in the following three sections, starting from a na\"{\i}ve approach that simply draws AOIs dynamically around visualization objects, to an `intelligent' one that detects objects more accurately by using knowledge about how specific visualizations may be typically used.  

\subsubsection{AOI-based AutomaticViewed Object Detection}
A na\"{\i}ve viewed object detection algorithm is to treat object shapes as dynamic AOIs, bin incoming gazes into those AOIs, and determine that the viewed object is that with most gazes in a recent time window, or the most recent fixation, landing in its AOI. This is a natural approach given that manually drawn AOIs are typically used in the same manner in eye-tracking data analysis, and that the similar concept of objects of interest (OOIs) has been proposed already by Papenmeier et al.~\cite{papenmeier2010dynaoi} for generative 3D content.

The problem with this approach is that for highly granular visualization content such as individual nodes or labels, users often fixate in the vicinity of the object rather than on the object itself. We demonstrate and quantify this observation in our evaluation section. A potential solution to this problem could be to make object AOIs slightly larger than the objects themselves. However, larger AOIs may lead to AOI overlaps in cluttered visualizations. Ultimately, the problem lies with an inability to determine with absolute certainty what a user is looking at, and is described in more detail in the next section.

\subsubsection{A probabilistic approach to viewed object detection}\label{sec:ProbabilisticObjectDetection}
Unlike mouse input, eye-tracking can only indicate a small screen region that a user is fixating, rather than a particular pixel. Typically, such a region is about one inch in diameter, though specific values depend on the user's distance to the monitor, and is determined by how human vision works. As such, we argue that it is generally impossible to tell with absolute certainty which object a user is viewing, if the user is fixating in the vicinity of multiple close objects (Figure~\ref{fig:discreminateFig4}(a)). This is not a significant problem for traditional AOI analyses, which generally use large AOIs. However, our goal is to detect the viewing of granular visualization content, such as network nodes or glyphs. 

\begin{figure}[htb]
  \centering
  \includegraphics[width=\linewidth]{images/discreminateFig4.eps}
  \caption{(a) An example real visualization where fixations are in the vicinity of multiple close objects. (b) ``Intelligent gaze interpretation'' is used to pin-point which object is most likely to be viewed, when gazes fall in the proximity of multiple objects ($O_3$ and $O_4$). Four objects $O_{1..4}$ are shown on the screen, two of which are connected ($O_1$,$O_3$), and one of which is highlighted ($O_3$). Given that at a previous moment time, $O_1$ or $O_2$ may have been viewed ($vs1 = 0.6$, $vs2=0.6$), that $O_1$ is connected to $O_3$, and that $O_3$ is highlighted, we augmented a raw gaze scores $gs$, computed from the gazes position, with  a prediction $ps$ that $O_3$ is more likely to be viewed than $O_4$.}
	\label{fig:discreminateFig4}
\end{figure}

As such, we advocate for a fuzzy interpretation of gaze data and detecting likelihoods that objects are viewed rather than certainties. To this end, we can compute object gazes scores $gs$ that range between zero- the object is not viewed, and one- the object is certainly viewed as shown in Figure~\ref{fig:gazeScoreFig3}. We note that the region of radius $R$ is similar to the concept of a user's foveated region, and as such needs to be constant in screen space. If the view is zoomed in or out, $R$ needs to be scaled accordingly in model space to remain constant in screen space.  A more detailed discussion about choosing and appropriate $R$ is given in the discussion section [REMINDER]. Finally, we mention that similar approaches were used by Salvucci et al.~\cite{salvucci2000intelligent} and Okoe et al.~\cite{okoe2014gaze}.
\begin{figure}[htb]
  \centering
  \includegraphics[width=\linewidth]{images/gazeScoreFig3.eps}
  \caption{The formula for calculating gaze score $gs$, where $d$ is the distance from the label to the gaze point $g(x,y)$ and $R$ is the radius of the fovea region. }
	\label{fig:gazeScoreFig3}
\end{figure}

\subsubsection{An intelligent algorithm for viewed object detection in data visualization}
Salvucci and Anderson described the concept of ``intelligent gaze interpretation'' in the context of a gaze-activated interface ~\cite{salvucci2000intelligent}. Their method achieves better accuracy in detecting which interface control a user is gazing at by integrating both the proximity of the gaze to the control, and the likelihood that the control would be the target of a gaze-interaction, given the history of previous interactions and the state of the interface. Formally, their algorithm identifies the most likely currently viewed item $i_{viewed}$ by solving $i_{best} = \displaystyle \argmax_{i\in I}[Pr(g|i)\cdot Pr(i)]$ , where $Pr(g|i)$ is the probability of producing a gaze at location g given the intention of viewing item $i$, and Pr(i) is the prior probability of an item $i$  being the target of a gaze-interaction. In Salvucci and Anderson's proof of concept implementation these prior probabilities were based on assumptions about how an interface might be used and were hardcoded into the system.  

We adapt Salvucci and Anderson's paradigm to increase the accuracy of determining which particular object a user might be viewing when a gaze-sample is registered in the vicinity of multiple objects. For example, we may assume that in a network visualization, a user who has just viewed a node $n$ will more likely view one of $n$'s neighbors than a random other node, perhaps especially if the user previously highlighted node $n$ and its outgoing edges.  

We exemplify this scenario in Figure~\ref{fig:discreminateFig4}(b): four visual objects ($O_{1\ldots 4}$), two of which are connected ($O_1$ and $O_3$), and one of which is highlighted ($O_3$), are shown on the screen. A new gaze sample registers between $O_3$ and $O_4$. Intuitively, it is more likely that $O_3$ was viewed since it is highlighted. Moreover, if we knew that $O_1$ was viewed just before the current moment, and, as described above we assume that users generally view neighboring nodes together, then this likelihood becomes even stronger.  An example from a real visualization is shown in Figure~\ref{fig:discreminateFig4} (a).

Formally, we compute a viewing score $vs$ of object $i$ at time $t$ ($vs(i,t)$) by weighing the gaze score $gs(i,t)$ described in Section~\ref{sec:ProbabilisticObjectDetection} by a prediction score $ps(i,t)$ that object $i$ is a viewing target at time $t$ (Formula~\ref{eq:ps}). This prediction score is computed based on the likelihood that the object is viewed given the current state of the visualization (e.g., the object is highlighted), and the likelihood that it is viewed if some other specific object was viewed just before it (e.g., a node's neighbor was viewed previously). Those two components are formalized by the  $\alpha$ score and $\beta$ score in Formula~\ref{eq:beta}.

\begin{equation}
gs_{i,t} = 1 - \min (1, (\frac{d}{R}))
\label{eq:GS}
\end{equation}

\begin{equation}
vs_{i,t} = gs_{i,t} \times ps_{i,t}
\label{eq:VS}
\end{equation}

\begin{equation}
ps_{i,t} = \alpha_{i,t} \times \beta_{i,t}
\label{eq:ps}
\end{equation}

\begin{equation}
\beta_{i,t} = \frac{\displaystyle\sum_{j} {vs_{j,t-1} \times T(j,i)}}{\displaystyle\sum_{j} vs_{j,t-1}} \text{ , where  } \parbox{15em}{$0\geq i \geq n$ and $gs_{i,t} \neq 0$\\ $0\geq j \geq n$ and $gs_{j,t} \neq 0$}
\label{eq:beta}
\end{equation}


We will assume $\alpha$ is given as an input to our algorithm. We will compute $\beta$ based on a viewing transition function between object - $T(i,j)$ gives the likelihood that object j is viewed after object $i$ is viewed – which we will assume is given as input to our algorithm. We could consider $ps_{j,t} = T(i,j)$ but that would involve knowing $i$, the previously viewed object. This is problematic because unlike Salvucci and Anderson, we wish to avoid making an unequivocal determination of which precise item was viewed at any given time, and instead aim to consider all objects with non-zero viewing scores as potentially viewed. In other words, we don't always know for sure what the previously viewed element was. As illustrated in Figure~\ref{fig:discreminateFig4}(b), $O_1$'s previous viewing score ($vs_{1,t-1}=0.6$), is just slightly larger than $O_2$'s viewing score ($vs_{2,t-1}=0.5$), and thus an absolute choice of $O_1$ over $O_2$ as previously viewed element would be rather arbitrary. 

To avoid this, we use a weighted average of transition probabilities from $O_1$ and $O_2$ to $O_3$ and $O_4$, using the previous view scores of $O_1$ and $O_2$ as weights.  More generally, $\beta_{i,t}$ is computed as shown in Formula~\ref{eq:beta}. In essence, the previously viewed objects $O_1$ and $O_2$ act as referees with varying degrees of influence in a competition between $O_3$ and $O_4$. This analogy, provides an intuition for an additional important guideline: an object should not referee a competition that it is part of. For example, using $O_3$ as a previous element in a competition between itself and $O_4$ would result in an open feedback-loop and should be avoided. This restriction is reflected in Formula~\ref{eq:beta}. 

\begin{algorithm}
\caption{Viewed Object Detection Algorithm}
\label{alg:ObjectDetection}
\begin{algorithmic}[1]
\State \textbf{Inputs: } 
\Statex $O_{i, \ldots, n}$= tracked visualization objects ( shapes, positions)
\Statex $g(x,y) = $ gaze sample in model space ( time $t$)
\Statex $\alpha_{i, \ldots, n} = $ viewing probability ($\alpha_{i, \ldots, n} \in [0,1]$)
\Statex $T(i,j) = $ viewing transition($T(i,j) \in [0,1]$)
\State \textbf{Outputs:}
\Statex $vs_{i,t} = $ momentary viewing scores of all objects ($i = 1, \ldots, n$). 
\For{$i \gets 1 \text{ to } n$}
	\State $gs_{i,t} \gets 1 - \min (1, (\frac{d}{R}))$	
\EndFor
\State $max \gets 0$
\For{$i \gets 1 \text{ to } n$}
	\If{$gs_{i,t} > 0$}
		\State $\beta_{i,t} = \frac{vs_{j,t-1} \times T(i,j)}{vs_{j,t-1}}$
		\State $ps'_{i,t} \gets \alpha_{i,t} \times \beta_{i,t}$
		\If{$ps'_{i,t} > max$}
			\State $max \gets ps'_{i,t}$
		\EndIf
	\EndIf
\EndFor
\For{$i \gets 1 \text{ to } n$}
	\State $vs_{i,t} \gets gs_{i,t} \times \frac{ps'_{i,t}}{max} $
\EndFor
\end{algorithmic}
\end{algorithm}

Two more factors need to be considered. First, to optimize for speed, we only compute prediction scores for objects with non-zero gazes. Second, we compute viewing scores for every gaze sample, rather than every fixation. We believe that doing so leads to results that are less dependent on how fixations are computed and more robust. Since our eye-tracker's sampling rate is $60$Hz, the scores $vs(t-1)$ indicate objects that were `viewed' just $15$ms ago, an interval much shorter than the time it takes for people to shift their attention to a new object. As such, instead of using the raw $vs(t-1)$ score, we use an average of the last several viewing scores. For all practical purposes, the term $vs(t-1,Oj)$ should be replaced in the previous formulas by $ \sum_{k=1}^{k=20}{vs(t-1, O_j)}$. The algorithm pseudocode is provided in Algorithm~\ref{alg:ObjectDetection}.



\subsection{Instrumenting a visualization}

\begin{figure}[htb]
  \centering
  \includegraphics[width=\linewidth]{images/pivotpaths.eps}
  \caption{PivotPaths visualization of IMDB data. Movies are displayed in the center of the scree, actors at the top, and directors and genres share the bottom space. Actors, directors, and genres associated to movies are connected through curves. Users can highlight objects and connections by hovering over them.}
	\label{fig:pivotpaths}
\end{figure}
We have use the previously described principles to instrument Doerk's interactive PivotPaths visualization of multifaceted data~\cite{dork2012pivotpaths}, which we linked to the popular internet movie database (IMDB). Shown in Figure~\ref{fig:pivotpaths}, the visualization renders movies in the center of the screen, actors on top, and genres and directors at the bottom. Actors, directors, and genres are connected by curves to the movies they associate with, and are larger, and their connections more salient, if they are associated with multiple movies. Actors, genres, and directors are colored distinctively, which is particular important for genres and directors since they occupy the same visual space. Such views are created in response to users' searches for specific movies, actors, and directors, and show data that is most relevant to the search. As shown in Figure~\ref{fig:pivotpaths}, users can hover over visual elements to highlight them and their connections. Users can also click on visual elements to transition the view to one centered on the select element. Finally, users can freely zoom and pan the visualization. 

To apply the previously described instrumentation method to this visualization, we had to define visual probabilities for objects, and transition probabilities between objects. One way to do this is by making informal assumptions about how the visualization may be directing users' visual patterns, a method employed by Salvucci~\cite{salvucci2000intelligent}. For instance, transitions between connected items may occur more often. It is reasonable to assume this, both because of the visual cue of a connecting line (i.e., users want to look at connected visuals together), but also because of the underlying data relation that the connection stands for (i.e., users want to look at connected data together). Also, transitions between objects of the same type, for instance from genre to genre, may be more dominant, especially in scanning processes. Finally, we can assume that elements that are hovered or highlighted are more likely to be viewed than those that are not, again, both because of a visual imperative (i.e., the eye is attracted to highlighted elements) and a task or data cue (i.e., the user already expressed interest in the data).

A more principled way to determine typical viewing patterns and sequences in a specific visualization is to run a preliminary eye-tracking study. In our case, we were able to leverage data collected from a pilot study, in which we instrumented an initial version of our PivotPaths visualization and collected viewing scores that were computed using gaze-coordinates alone (i.e., without including prediction scores). Analyzing the resulting data by summarizing what objects were typically viewed before other objects confirmed the assumptions we made informally (Figure~\ref{fig:transitions}, top) (e.g., transitions between connected elements are between $1.5$ and $6$ times more likely than between unconnected elements). 

Based on this information, we defined the transition and viewing probabilities as shown in Figure~\ref{fig:transitions}, bottom. These transition probabilities do not match directly the observed probabilities for the following reason. First, as can be seen in Figure~\ref{fig:pivotpaths}, movies, actors, and directors and genres occupy separate spaces. As such, it is unlikely that we will need to discriminate between a movie and an actor, or between a movie and a director. As such, we only care about how these probabilities compare to each other within the actor segment, the movie segment, and the director plus genre segment. Second, while we refer to the values in Figure~\ref{fig:transitions} as probabilities, they don't need to add to one because of the way we compute scores. 

\begin{figure}[htb]
  \centering
  \includegraphics[width=0.75\linewidth]{images/transitions.eps}
  \caption{Transition and viewing probabilities in our instrumented visualization system (bottom two table) were informed by real patterns observed in a preliminary eye tracking study (top). }
	\label{fig:transitions}
\end{figure}

Finally, as part of instrumentation, our system collected application screen shots, mouse input events and states (e.g., cursor position, mouse press, mouse release), interactive events (e.g., hovering, zooming, panning), raw gaze samples captured at a rate of $60$Hz, and visual elements with non-zero viewing scores also captured a rate of $60$Hz, since new viewing scores are computed for every gaze sample. For each viewed element we recorded the type (i.e., movie, actor, director, genre), its gaze score ($gs$), it's prediction score ($ps$), and the aggregated viewing score ($vs$). All recorded data was time stamped. 

\subsection{Analyzing collected data}

We built a dedicated visualization system to analyze the data that our instrumentation allowed us to collect (Figure~\ref{fig:system}). We note that our main goal was to test the validity and effectiveness of capturing eye-tracking data in visualization and data space, rather than to explore generalizable and efficient techniques of analyzing such data.  

\begin{figure}[htb]
  \centering
  \includegraphics[width=\linewidth]{images/system.eps}
  \caption{A visualization system allowed to explore eye-tracking data in visualization space. A heatmap visualization at the bottom summarized which objects were viewed (left, vertically) over time (top, horizontally). Analysis could be restricted to a temporal interval by dragging start and end time lines (green and pink vertical lines). A particular moment in time could be selected (grey vertical time line) for which a screen shot of the user's activity was displayed, along with where they were looking at that time (red dot). }
	\label{fig:system}
\end{figure}

We show object viewing activity over time using a heatmap representation, by discretizing time and displaying it horizontally, and listing viewed objects vertically. The time intervals used for discretization, and the pixel size of heatmap cell could be changed interactively, thus allowing us to explore the data at multiple levels of temporal granularity. The viewed objects listed vertically were colored based on their type (movie, actor, director, genre) and could be sorted based on either first time they were viewed, amount of viewing activity, or type. 

Showing viewing data at multiple time scales requires a method of aggregating viewing scores collected every $15$ms into larger time intervals. While it may be interesting to explore the effects of different aggregation strategies, our current system only implemented one. We use Formula~\ref{eq:Aggregate} to compute the aggregated score of $O_i$ in time interval $T$. Intuitively, the formula averages all non-zero viewing scores collected in interval $T$, but also takes into account the duration of $T$. For instance, six high viewing scores recorded in a $100$ms time span are sufficient to yield a high aggregated score for that interval ( $\frac{100 \times 60 }{1000} = 6$), while for a $1000$ms interval we would require $60$ high viewing scores ( $\frac{1000 \times 60 }{1000} = 60$).
\begin{equation}
aggScore(T,O_i) = \frac{\displaystyle\sum_{t \in T }{vs(t, O_i)}}{\max (count(vs(t, O_i) > 0 , t \in T),|T| \times \frac{60}{1000}) }
\label{eq:Aggregate}
\end{equation}

The heatmap could be cleaned by filtering cells and rows. Cells could be filtered based on their value relative to the average value of all non-zero cells in the heatmap; the values of filtered cells were set to zero. Similarly, a row could be filtered depending on how its average value across all time intervals compared to such averages of other rows. Filtered rows were not shown in the heatmap. 

The system also allowed us to select a specific temporal subset of all collected data that the heatmap should display, by interactively dragging two vertical bars indicating the start and end time of the desired temporal interval. Once such an interval was defined or changed, the heatmap was recomputed to only include visual objects that were viewed in that interval. Restricting the time interval that the heatmap depicted was also reflected in filtering operations. As such, the heatmap could be configured to show objects most viewed in a temporal subset of the data, which could differ from objects most viewed in the data set as a whole.

We show interaction data as additional horizontal bars on top of the heatmap visualization. Figure~\ref{fig:system} exemplifies this: the black and grey strip indicate a change in view (e.g., new search for an actor), the pink strip indicates hovering events, and the blue strip indicates zooming or panning. 

The system allowed us to select a specific moment in time by dragging a vertical bar across the width of the heatmap. Selecting a moment in time would display a screen shot of what the user was viewing at that time, overlaid with gaze samples closest to that time point. 

Finally, the system allowed us to load data from multiple users at the same time and display views of either users' individual data, or users' aggregated data. However, aggregated user results were only computed and shown for viewing activity, by averaging user heatmaps over time. Seeing screen-shots was also disabled in aggregated mode.